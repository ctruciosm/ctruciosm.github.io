[
  {
    "path": "posts/2021-06-18-criar-um-novo-projetorepo-no-rstudiogithub/",
    "title": "Criar um novo projeto/repo no Rstudio/GitHub",
    "description": "Passo a passo para criar um novo projeto no Rstudio e vinculá-lo ao GitHub.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-06-18",
    "categories": [
      "Rstudio",
      "Git",
      "GitHub"
    ],
    "contents": "\n\nContents\nIntrodução\nPré-requisitos.\nCriando um novo projeto.\n\n\n\n\nIntrodução\nNo meu post anterior, falei um pouco de como configurar o Rstudio para ele estar vinculado ao nosso Github. Hoje explicarei como criar um novo projeto no Rstudio e vinculá-lo ao GitHub (sem precisar clonar um repositório existente como vimos no post anterior).\nPré-requisitos.\nTer sua conta do Git/GitHub configurada com o Rstudio.\nCriando um novo projeto.\nPara criar um novo projeto:\n\n\nusethis::create_project(\"/Volumes/CTRUCIOS_SD/Research/repo_teste\")\n\n\n\nrepare que /Volumes/CTRUCIOS_SD/Research/ é o local onde quero criar o novo projeto e repo_teste é o nome do novo projeto.\nAssim que rodar o código acima, aparecerá algo parecido com:\n\n\n\nLogo após, o Rstudio abrirá outra sessão com o novo projeto. Se não abriu, vá na pasta onde criou o projeto, no meu caso /Volumes/CTRUCIOS_SD/Research/, e encontrará uma pasta com o nome repo_teste onde encontrará o projeto do Rstudio (arquivo repo_teste.Rproj).\nNessa nova sessão do Rstudio, repare que o nome do novo projeto aparece no canto superior direito. (Aqui já pode fechar todas as outras sessões abertas do Rstudio e ficar apenas com a do nosso novo projeto).\n\n\n\nAgora precisamos vincular esse novo projeto ao Git/GitHub. Primeiro vincularemos o projetoco Git.\n\n\nusethis::use_git() \n\n\n\n\n\n\n\n\n\nUma vez vinculado o projeto ao Git, vinculamos ele ao GitHub.\n\n\nusethis::use_github()\n\n\n\n\n\n\nSe tudo de certo, vá no teu Github e terá o repositório lá.\n\n\n\nPronto! Seu novo projeto foi criado, e vinculado com o Git/GitHub.\nHappy Coding!\n\n\n\n",
    "preview": "posts/2021-06-18-criar-um-novo-projetorepo-no-rstudiogithub/im/original.png",
    "last_modified": "2021-06-18T12:11:27-03:00",
    "input_file": {},
    "preview_width": 896,
    "preview_height": 896
  },
  {
    "path": "posts/2021-06-12-git-github-e-rstudio/",
    "title": "Git, GitHub e Rstudio",
    "description": "Tutorial de como configurar o Git/GitHub no Rstudio.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-06-12",
    "categories": [
      "Rstudio",
      "Git",
      "GitHub"
    ],
    "contents": "\n\nContents\nIntrodução\nPré-requisitos\nPrimeiros Passos\nIniciando a configuração\nVinculando um repositório ao Rstudio\nBonus: commit and push\n\nIntrodução\nSe você escreve código (seja na linguagem que for) sabe que utilizar o Git/GitHub é muito útil. Se você (assim como eu) gosta de usar o Rstudio, talvez esteja interessado(a) em configurar sua conta do Git no Rstudio.\nEsta configuração é algo que é feito apenas uma única vez na vida (ok não na vida, mas uma única vez por computador) e este tutorial vai te guiar no passo a passo.\nPré-requisitos\nAntes de começar, existem algumas coisas que devemos fazer antes (caso ainda não tenha feito):\nInstalar o R (Baixar aqui)\nInstalar o Rstudio (Baixar aqui)\nCriar uma conta no Github (criar conta aqui)\nInstalar o Git (Baixar aqui)\nPrimeiros Passos\nNo Rstudio instalar e carregar o pacote usethis\n\n\ninstall.packages(\"usethis\")\nlibrary(usethis)\n\n\n\nPara configurar sua conta do Git no Rstudio precisará de um nome de usuário e do email utilizado na conta do Github.\n\n\nusethis::use_git_config(user.name = \"nome_usuario_aqui\", \n                        user.email = \"email_github_aqui@gmail.com\") \n\n\n\nIniciando a configuração\nNo Rstudio escreva\n\n\nusethis::create_github_token()\n\n\n\nisto abrirá no seu navegador algo mais ou menos assim:\n\n\n\nVocê pode escrever o nome que você quiser (por exemplo RstudioNotebook, RstudioCasa, etc) mas recomendo que seja um nome que lhe ajude a lembrar para que esse token é utilizado. Eu deixei todos os valores por padrão e no final da página basta clicar em Generate token.\n\n\n\nLogo após, aparecera o token, o qual você precisa copiar (click no icone azul bem do lado do token). Esse token será necessário para terminar a configuração do Git com o Rstudio.\n\n\n\nAgora escreva\n\n\nusethis::edit_r_environ()\n\n\n\nO Rstudio abrirá um arquivo chamado .Renviron, nele escreva o token que acabou de ser gerado e depois dê um enter (para pular linha).\n\n\n\nSalve o arquivo e reinicie sessão (se preferir pode fechar o Rstudio e abri-lo de novo)\n\n\n.rs.restartR()\n\n\n\n\nPronto! Agora já está configurado.\n\nVinculando um repositório ao Rstudio\nA forma mais fácil de trabalhar com um projeto no Rstudio e que este esteja vinculado ao GitHub é clonando um reposítorio existente. Para isto, precisamos primeiro criar um repositório no GitHub e depois cloná-lo.\nCriando o repositório no GitHUb\nNo GitHub, vá na aba Repositories e dê clic em New\n\n\n\nAparecerá a janela abaixo e você pode escolher se criar um repositório público ou privado e se adicionar um README ou nao. Aqui, eu escolhi a opção privado e adicionar README. Após escolher as opções da sua preferêcia clic em Create repository.\n\n\n\nCom o repositório criado, dê um clic na setinha do botão Code e copie o endereço que aparecer.\n\n\n\nClonando o repositorio.\nAgora estamos chegando na etapa final. Abra o Rstudio e crie um novo projeto File > New Project... e escolha a opção Version Control.\n\n\n\nEnseguida escolha a opção Git e preencha a opção Repository URL com o endereço que acabou de copiar do GitHub. Escolha também onde quer criar o projeto no seu computador (no meu caso /Volumes/CTRUCIOS_SD/Research). Não esqueça marcar a opção Open in new session.\n\n\n\n\n\n\nClicar em Create project e Pronto! Seu repositório no GitHub e seu novo projeto do Rstudio estão vinculados, agora é só escrever seus códigos e fazer commits, Push and Pulls.\nBonus: commit and push\nO código que você escreverá no seu computador, ficará unicamente no seu computador. Por isso, não esqueça fazer commit e Push de tempos em tempos para salvar no GitHub seu código mais recente.\nPara mostrar como se fazer commit and Push, criei um arquivo chamado codigo01.R\n\n\n\nPara fazer commit, vá na aba Git e dê clic na opção Commit.\n\n\n\nAgora marque os arquivos que quer subir para o GitHub (no nosso caso, o arquivo codigo01.R) e escreva uma breve descrição do que foi feito no código (para você lembrar).\n\n\n\nClic em Commit e depois clic em Push.\nSe tudo deu certo, seu código aparecerá no seu repositório do GitHub.\n\n\n\nNota: Se o arquivo codigo01.R não aparecer no GitHub, tente fazer um Push de novo, se lhe pedir usuario e senha, coloque seu nome de usuário do GitHub e quando pedir a senha coloque o token que foi gerado (no começo deste post).\n\n\n\nEspero ter ajudado. Happy Coding!\n\n\n\n",
    "preview": "posts/2021-06-12-git-github-e-rstudio/im/001.png",
    "last_modified": "2021-06-17T21:32:54-03:00",
    "input_file": {},
    "preview_width": 1340,
    "preview_height": 688
  },
  {
    "path": "posts/2021-05-01-configurando-rstudio-para-usar-o-rmarkdown/",
    "title": "Configurando Rstudio para usar o Rmarkdown",
    "description": "Passo a passo de como configurar o Rstudio para fazer relatórios e apresentações utlizando o Rmarkdown.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-05-01",
    "categories": [
      "Rstudio",
      "Rmarkdown"
    ],
    "contents": "\n\nContents\nIntrodução\nPasso a Passo\nConclusões\n\nIntrodução\nFazer relatórios técnicos, apresentações e escrever artigos é parte do dia a dia tanto das pessoas envolvidas com pesquisa quanto das envolvidas com análise de dados.\nFazer estas tarefas pode ser cansativo e, inclussíve, levar a erros na digitação dos resultados.\nUma forma eficiente e fácil de fazermos isto é misturar a funcionalidade do R com a versatilidade de fazer relatórios utilizando o Rmarkdown.\nA seguir, descreverei passo a passo como configurar o Rstudio para poder fazer apresentações/relatórios utilizando o Rmarkdown1.\nPasso a Passo\nSe é a primeira vez que está tentando fazer um relatório/apresentação e está em dúvida sobre como proseguir, segue um passo e passo detalhado\nPasso 1\nQuando abrir o Rstudio, verá uma tela semelhante com essa aqui:\n\n\n\nPasso 2\nCrie um script markdown:\n\nFile > New File > R Markdown\n\n\n\n\nAssim que criar o script aparecerá a seguinte mensagem\n\n\n\nbasta clicar em Yes e deixar que todos os pacotes sejam instalados (você verá algo como a tela a seguir).\n\n\n\nPasso 3\nApós instalar todos os pacotes, tente mais uma vez criar o script\n\nFile > New File > R Markdown\n\nPor padrão, a opção Document estará ativa e se tudo estiver certo, você verá na tela o seguinte:\n\n\n\nse você escolher a opção Presentation, verá a seguinte:\n\n\n\nEm ambas as opções você pode escolher como fará o relatório/apresentação (html, pdf ou word/power point). Escolha a opção PDF ou PDF (Beamer)2\nPasso 4\nIndependente da opção escolhida (Document ou Presentation) aparecerá um template pronto para rodar. Basta você clicar no botão Knit\n\n\n\n(esse botão é para compilar tudo o escrito no seu relatório/apresentação e utilizará ele frequentemente)\nSe você for usuário de Latex, já deve ter o miktex instalado e o Rmarkdown deveria funcionar normalmente. Caso não seja usuário de Latex, continue com os próximos passos.\nPasso 5\nQuando apertar o botão pela primeira vez, caso você não tenha instalado antes o miktex, o arquivo não compilará e aparecerá algo parecido com:\n\n\n\nO que importa para nós é a seguinte mensagem:\n\n\n\nOu seja, precisamos instalar mais algumas coisinhas antes de podermos fazer apresentações/relatórios utilizando o Rstudio.\nBasta rodar o comando\n\n\ntinytex::install_tinytex()\n\n\n\nque tudo o necessário será instalado (você verá uma janela como a seguinte)\n\n\n\nPasso 6\nAssim que a instalação anterior tiver terminado, aparecerá a seguinte mensagem\n\n\n\nmas não se preocupe, apenas ignore.\nPasso 7\nPronto! já pode utilizar o Rstudio para fazer relatórios. Para testar, aperte o botão knit\n\n\n\ne o pdf de exemplo será compilado!\n\n\n\nConclusões\nRstudio é uma excelente IDE para se trabalhar com R, mas agora podemos além de escrever nosso código, fazer relatórios/apresentações\nPara poder utilizar basta apenas seguir alguns passos simples de configuração\nSe você já tiver instalado o Latex no seu computador, pode pular os passos 5 e 6, e ir direto para o passo 7\nCaso não saiba o que é Latex (isso significa que não tem o Latex nem miktex instalados) não se preocupe, basta seguir os passos 5 e 6 que tudo ficará pronto para utilizar o Rstudio para fazer relatórios/apresentações\n\nOs passos são bem intuitivos e a maioria de pessoas talvez não precise deste tutorial, mas, caso alguém precisar, deixo aqui o passo a passo↩︎\nPode escolher qualquer outra opção mas eu prefiro fazer em PDF. Ah, eu nunca testei a opção word/power point mas deveria funcionar↩︎\n",
    "preview": "posts/2021-05-01-configurando-rstudio-para-usar-o-rmarkdown/im/001.png",
    "last_modified": "2021-06-17T21:32:39-03:00",
    "input_file": {},
    "preview_width": 1366,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-01-minimos-quadrados-ordinarios/",
    "title": "Mínimos Quadrados Ordinários",
    "description": "Quando trabalhamos com modelos de regressão é necessário estimar os parâmetros do modelo. Um dos métodos de estimação mais conhecidos é o estimador de mínimos quadrados ordinários (ou MQO para os amigos). Em este post discutiremos a necessidade de estimar os parâmetros, entendermos a intuição por trás do método e derivaremos, passo a passo, o estimador MQO no modelo de regressão linear simples.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-04-01",
    "categories": [
      "Linear Regression",
      "Proof"
    ],
    "contents": "\n\nContents\nIntrodução\nA intuição por trás do estimador MQO\nDerivando o estimador MQO\nPrimeira derivada\nIgualando a zero\nVerificando que é ponto de mínimo\nEstimador MQO\n\nConclusões\n\nIntrodução\nO modelo de regressão linear simples é um dos modelos mais simples de Statistical Learning / Econometria. O modelo assume que a relação entre as variáveis \\(Y\\) e \\(X\\) é dada por: \\[\\begin{equation}\\label{RLS}\nY = \\beta_0 + \\beta_1 X + u\n\\end{equation}\\] em que \\(\\beta_0\\) e \\(\\beta_1\\) são os parâmetros do modelo e \\(u\\) é um termo aleatório.\nSe conhecermos \\(\\beta_0\\) e \\(\\beta_1\\), basta utilizar o modelo para um determinado valor de \\(X\\), digamos \\(x\\), e saberemos o valor esperado de \\(Y\\) dado \\(x\\)1.\nInfelizmente, nunca conhecemos \\(\\beta_0\\) e \\(\\beta_1\\) e então precisamos estimar esses valores utilizando os dados da nossa amostra.\nExistem vários métodos de estimação, mas hoje discutiremos o estimador de mínimos quadrados ordinários (MQO).\nA intuição por trás do estimador MQO\nSejam \\((y_1, x_1), \\ldots, (y_n, x_n)\\) os elementos de uma amostra aleaatória (a.a) de tamanho \\(n\\) extraida de \\((Y,X)\\). Estamos interessados em estimar os parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) na reta de regressão, mas…qual reta escolher?\nA Figura 1 apresenta o gráfico de dispersão de educação (\\(x\\)) vs. renda (\\(y\\)). Como podemos ver, existem várias retas (na verdade existem infinitas retas) que podemos tracejar na nossa tentativa de estimar \\(\\beta_0\\) e \\(\\beta_1\\).\n\n\n\nFigure 1: Gráfico de dispersão education vs. income\n\n\n\nEntre os vários critérios que poderíamos escolher para escolher a reta, vamos escolher a reta que minimiza a soma de quadrados dos resíduos. Mas… quem são esses resíduos? os resíduos, denotados por \\(\\hat{u}_i\\), são definidos como \\(\\hat{u}_i = y_i - \\hat{y_i}\\) com \\(\\hat{y}_i = b_0 + b_1x_i\\). Ou seja, independente dos valores \\(b_0\\) e \\(b_1\\) que escolhermos, os resíduos são a diferença entre o valor verdadeiro (\\(y_i\\)) e o estimado (\\(\\hat{y}_i\\)).\nOs valores \\(b_0\\) e \\(b_1\\) que minimizem a SQR vamos denotá-los como \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\). Em termos matemáticos, queremos \\(b_0\\) e \\(b_1\\) tal que \\[\\hat{\\beta}_0, \\hat{\\beta}_1 = \\mathop{\\mathrm{argmin}}\\limits_{b_0, b_1} SQR\\] em que \\[SQR := \\displaystyle \\sum_{i=1}^n \\hat{u}_i^2 \\equiv \\displaystyle \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\equiv \\displaystyle \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\\]\nDerivando o estimador MQO\nNosso problema de encontrar \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) resume-se, então, a um problema de minimização, onde a função a minimizar é \\[SQR := \\displaystyle \\sum_{i=1}^n (\\underbrace{y_i - b_0 - b_1 x_i}_{\\hat{u}_i})^2\\]\nPara resolver nosso problema de minimização vamos a lembrar um pouco das aulas de cálculo. Vamos a calcular a primeira derivada, igualar a zero para obter os valores candidatos e finalmente, vamos a calcular a segunda derivada para verificar que efetivamente os valores encontrados são valores que minimizam a função.\nPrimeira derivada\nDerivando SQR em relação a \\(b_0\\) temos\n\\[\\begin{equation}\n\\dfrac{\\partial SQR}{\\partial b_0}   = -2 \\displaystyle \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)  = -2 n \\big( \\bar{y} - b_0 - b_1 \\bar{x} \\big).\n\\end{equation}\\]\nDerivando SQR em relação a \\(b_1\\) temos\n\\[\\begin{equation}\n\\dfrac{\\partial SQR}{\\partial b_1}= -2 \\displaystyle \\sum_{i=1}^n x_i (y_i - b_0 - b_1 x_i) = -2  \\Big( \\displaystyle \\sum_{i=1}^n x_i y_i - b_0 \\displaystyle \\sum_{i=1}^n x_i - b_1 \\displaystyle \\sum_{i=1}^n x_i^2 \\Big).\n\\end{equation}\\]\nIgualando a zero\nFazendo \\(\\frac{\\partial SQR}{\\partial b_0} = 0\\) e \\(\\frac{\\partial SQR}{\\partial b_1} = 0\\) temos:\n\\[\\underbrace{\\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}}_{A} \\quad e \\quad \\underbrace{\\displaystyle \\sum_{i=1}^n x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0}_{B}\\]\n\nQuando igualamos a zero, substituimos \\(b_0\\) por \\(\\hat{\\beta}_0\\) e \\(b_1\\) por \\(\\hat{\\beta}_1\\) pois \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) são a solução do sistema.\nVamos então resolver o sistema de equações para obter \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\).\nDe \\(A\\) temos que \\(\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\). Agora vamos substituir \\(\\hat{\\beta}_0\\) em \\(B\\),\n\\[\\displaystyle \\sum_{i=1}^n x_i (y_i - \\underbrace{(\\bar{y} - \\hat{\\beta}_1 \\bar{x})}_{\\hat{\\beta}_0} - \\hat{\\beta}_1 x_i) = 0\\] \\[\\displaystyle \\sum_{i=1}^n x_i(y_i-\\bar{y}) - \\hat{\\beta}_1 \\sum_{i=1}^n x_i (x_i - \\bar{x}) = 0\\]\n\\[\\displaystyle \\sum_{i=1}^n x_i(y_i-\\bar{y}) = \\hat{\\beta}_1 \\sum_{i=1}^n x_i (x_i - \\bar{x}),\\] Então, \\[\\hat{\\beta}_1 = \\dfrac{\\displaystyle \\sum_{i=1}^n x_i(y_i-\\bar{y})}{\\displaystyle \\sum_{i=1}^n x_i (x_i - \\bar{x})}.\\] Provavelmente, a fórmula não se parece muito com as fórmulas que você está acostumado a ver nos livros. Vamos trabalhar um pouco com as expressões no numerador e denominador para chegarmos a uma fórmula um pouco mais conhecida.\nNo denominador:\n\\[\\displaystyle \\sum_{i=1}^n x_i (x_i-\\bar{x}) = \\sum_{i=1}^n (x_i - \\bar{x} + \\bar{x})(x_i-\\bar{x}) = \\underbrace{\\sum_{i=1}^n (x_i - \\bar{x})(x_i-\\bar{x})}_{\\displaystyle \\sum_{i=1}^n (x_i - \\bar{x})^2} + \\underbrace{\\sum_{i=1}^n  \\bar{x}(x_i-\\bar{x})}_{\\bar{x} \\displaystyle \\sum_{i=1}^n  (x_i-\\bar{x})},\\] e como \\(\\displaystyle \\sum_{i=1}^n (x_i - \\bar{x}) = \\underbrace{\\sum_{i=1}^n x_i}_{n\\bar{x}} - n \\bar{x} = 0,\\) temos que \\[\\displaystyle \\sum_{i=1}^n x_i (x_i-\\bar{x}) = \\displaystyle \\sum_{i=1}^n (x_i-\\bar{x})^2.\\] No numerador:\n\\[\\displaystyle \\sum_{i=1}^n x_i(y_i-\\bar{y}) = \\displaystyle \\sum_{i=1}^n (x_i - \\bar{x} + \\bar{x})(y_i-\\bar{y}) = \\displaystyle \\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y}) + \\underbrace{\\displaystyle \\sum_{i=1}^n \\bar{x}(y_i-\\bar{y})}_{\\bar{x} \\underbrace{\\displaystyle \\sum_{i=1}^n (y_i-\\bar{y})}_{0}}\\] Então temos que, \\[\\hat{\\beta}_1 = \\dfrac{\\displaystyle \\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\displaystyle \\sum_{i=1}^n (x_i-\\bar{x})^2} \\quad e \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\\]\nVerificando que é ponto de mínimo\nPara verificar que a solução obtida é ponto de mínimo, precisamos que a matriz Hessiana \\[\\begin{bmatrix} \n\\dfrac{\\partial^2 SQR}{\\partial b_0^2} & \\dfrac{\\partial^2 SQR}{\\partial b_0 \\partial b_1}  \\\\\n\\dfrac{\\partial^2 SQR}{\\partial b_1 \\partial b_0} & \\dfrac{\\partial^2 SQR}{\\partial b_1^2}  \\\\\n\\end{bmatrix}\\] seja definida positiva.\n\\(\\dfrac{\\partial^2 SQR}{\\partial b_0^2} = \\dfrac{\\partial}{\\partial b_0}\\dfrac{\\partial SQR}{\\partial b_0} = 2n\\)\n\\(\\dfrac{\\partial^2 SQR}{\\partial b_0 \\partial b_1} = \\dfrac{\\partial}{\\partial b_0}\\dfrac{\\partial SQR}{\\partial b_1} = 2n \\bar{x}\\)\n\\(\\dfrac{\\partial^2 SQR}{\\partial b_1 \\partial b_0} = \\dfrac{\\partial}{\\partial b_1}\\dfrac{\\partial SQR}{\\partial b_0} = 2n \\bar{x}\\)\n\\(\\dfrac{\\partial^2 SQR}{\\partial b_1^2} = \\dfrac{\\partial}{\\partial b_1}\\dfrac{\\partial SQR}{\\partial b_1} = 2 \\displaystyle \\sum_{i=1}^n x_i^2\\)\nLogo, a matriz Hessiana \\[\\begin{bmatrix} \n2n & 2n \\bar{x} \\\\\n2n \\bar{x} & 2 \\displaystyle \\sum_{i=1}^n x_i^2  \\\\\n\\end{bmatrix},\\] é definida positiva pois os elementos da diagonal são positivos e o determinante \\(4n\\displaystyle \\sum_{i=1}^n x_i^2 - 4n^2\\bar{x}^2 = 4n \\Big( \\sum_{i=1}^n x_i^2 - n\\bar{x}^2\\Big) = 4n \\displaystyle \\sum_{i=1}^n(x_i - \\bar{x})^2\\) é também positivo.\nEstimador MQO\nAssim, temos mostrado que os estimadores MQO para \\(\\beta_0\\) e \\(\\beta_1\\) no modelo de regressão linear simples são \\[\\hat{\\beta}_1 = \\dfrac{\\displaystyle \\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\displaystyle \\sum_{i=1}^n (x_i-\\bar{x})^2} \\quad e \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\\]\nConclusões\nO estimador MQO consiste em encontrar \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) que minimizm a soma de quadrados dos resíduos.\nUtilizando os critérios da primeira e segunda derivada temos obtido os estimadore MQO.\nSob algumas hipóteses, os estimadores MQO são o melhor estimador linear não viesado ver Teorem de Gauss-Markov\nSe quiser saber como implementar este modelo em R e Python pode ver este post.\n\nAssumindo que \\(\\mathbb{E}(u|X) = 0\\), \\(\\mathbb{E}(Y|x) = \\beta_0 + \\beta_1 x\\)↩︎\n",
    "preview": "posts/2021-04-01-minimos-quadrados-ordinarios/derivando-o-estimador-de-mqo_files/figure-html5/MQO1-1.png",
    "last_modified": "2021-06-17T21:32:08-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-14-book-review-r-for-data-science/",
    "title": "Book Review: R for Data Science",
    "description": "Minhas impressões do livro R for data science: import, tidy, transform, visualize, and model data do Hadley Wickham e Garrett Grolemund.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-03-14",
    "categories": [
      "Book Review",
      "R"
    ],
    "contents": "\n\nContents\nIntrodução\nOverview do livro\nO que menos gostei\nComentários Finais\n\nIntrodução\nQuem me conhece um pouco, sabe que eu não sou um grande fã de livros do tipo Hands-On blah blah, mas recentemente, buscando ajudar aos meus alunos na sua caminhada acadêmica, resolvi incluir alguns deles na minha lista de leitura. Espero que estes comentários sejam de ajuda, principalmente, para meus (ex-/atuais/futuros) alunos.\nOverview do livro\n\n\n\n\n\nR tem evoluído bastante desde que foi oficialmente lançado em 2001 e o livro R for data science: import, tidy, transform, visualize, and model data (Wickham and Grolemund 2016) faz um bom papel apresentando uma introdução ao R e à filosofia tidyverse1 de forma clara e direta. O pacote (ou, na verdade o conjunto de pacotes) tidyverse é sem dúvida a tendência hoje em dia, e qualquer pessoa que trabalha com dados o utilizará com frequência.\nSugiro que à medida que você for lendo o livro implemente os códigos que forem aparecendo, dessa forma você poderá ir mexendo gradualmente no código para ver o que acontece se… o que lhe ajudara no processo de aprendizagem.\nUm dos capítulos que mais gostei foi o capítulo 3, que apresenta de forma bem leve uma introdução ao pacote ggplot2 para visualização de dados. Um bom complemento para esse capítulo aparece quase no final do livro, no capítulo 28, onde se apresentam alguns detalhes sobre títulos, captions e nomes nos eixos. Se você tiver interesse em se aprofundar no ggplot2, a melhor fonte é o livro ggplot2: Elegant Graphics for Data Analysis (Hadley 2016) que está disponível online e de graça aqui.\nOs capítulos 9 – 16 apresentam bastante material sobre manipulação de dados, super útil para construir nossa ABT2. Contudo, creio que quem não está muito acostumado com o R ou com manipulação de dados pode ter uma overdose de informação. Não se preocupe tanto por entender tudo que está no livro, mas por entender o que pode ser feito com o R e com os pacotes dplyr,readr, lubridate, etc, incluidos no tidyverse. Existem diversos Cheatsheets que ajudam a lembrar como cada umas das funções discutidos nos capítos 9–16 funcionam, salve eles no computador e tenha-os sempre por perto.\nOutro capítulo que achei muito interessante é o capítulo 25 (mas para quem está iniciando eu recomendaria pular esse capitulo e voltar nele quando for um usuário de R mais frequente), ele apresenta informação valiosa para quem tem interesse em comparar vários modelos e colocar modelos em produção.\nO que menos gostei\nEmbora eu tenha desfrutado bastante meu tempo lendo o livro, achei os capítulos 22–24 meio confussos, principalmente para quem está iniciando. Quando se trata de modelagem , eu prefiro uma abordagem mais clássica onde se explica como o modelo é construido e quais são os princípios por tras dele, mas entendo que isso está completamente fora do escopo do livro.\nPara quem está começando, eu leria o capítulo 21 apenas até a seção 21.3. As seções 21.4 – 21.9 são importantes, mas eu deixaria elas para uma segunda leitura ou para quando estiver mais familiarizado com o R e com programação.\nComentários Finais\nResumindo, R for data science: import, tidy, transform, visualize, and model data (Wickham and Grolemund 2016) é um bom livro, completo e didático. Eu gostei da maioria de capítulos, com algumas poucas exceções.\nO livro tem uma versão em português, mas eu li a versão em inglês (que é disponibilizada gratuitamente pelos autores).\nSe tiver com dificuldade em resolver os exercícios do livro, Jeffrey B. Arnold providenciou um solucionário (eu não o li).\n\n\n\nHadley, Wickham. 2016. ggplot2: Elegrant Graphics for Data Analysis. Springer.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\".\n\n\nConjunto de pacotes que seguem a mesma filosofia tidy, para mais detalhes veja tidyverse.org↩︎\nABT: Analytical Base Table↩︎\n",
    "preview": "posts/2021-03-14-book-review-r-for-data-science/cover.png",
    "last_modified": "2021-06-17T21:31:33-03:00",
    "input_file": {},
    "preview_width": 500,
    "preview_height": 750
  },
  {
    "path": "posts/2021-02-28-teorema-de-gauss-markov/",
    "title": "Teorema de Gauss-Markov",
    "description": "Uma das propriedades mais interessentes dos estimadores MQO é fornecida pelo Teorema de Gauss-Markov. Neste post discutimos a importância, significado e fornecemos uma demostração passo a passo do Teorema.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-02-28",
    "categories": [
      "Linear Regression",
      "Proof"
    ],
    "contents": "\n\nContents\nIntrodução\nTeorema\nDemostração\nConclusão\n\nIntrodução\n\n\n\nO estimador de mínimos quadrados ordinários (MQO) é um dos métodos de estimação mais utilizados quanto à analise de regressão se refere. Ele é atrativo pela sua simplicidade e boas propriedades.\nSejam \\(\\{(y_i, x_{i,1}, \\ldots, x_{i,k}) \\}_{i=1, \\ldots, n}\\) tais que:\n\\[\\begin{align}\n\\begin{split}\\label{eq:1}\n    y_1 &= \\beta_0 + \\beta_1 x_{1,1}  + \\cdots + \\beta_k x_{1,k} + u_1 \\\\\n    \\vdots \\\\\n    y_n &= \\beta_0 + \\beta_1 x_{n,1}  + \\cdots + \\beta_k x_{n,k} + u_n \\\\\n\\end{split}\n\\end{align}\\]\nou equivalentemente\n\\[ \\underbrace{\\left[ \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right]}_{Y} = \\underbrace{\\begin{bmatrix} \n1 & x_{1,1} & \\cdots & x_{1,k} \\\\ \n\\vdots & \\vdots & \\cdots & \\vdots \\\\ \n1 & x_{n,1} & \\cdots & x_{n,k} \\end{bmatrix}}_{X} \\times \\underbrace{\\left[ \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right]}_{\\beta} + \\underbrace{\\left[ \\begin{array}{c} u_1 \\\\ \\vdots \\\\ u_n \\end{array} \\right]}_{u}\\]\nentão, o estimador MQO de \\(\\beta\\) é dado por \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\).\nSob certas condições, o Teorema de Gauss-Markov nos diz que \\(\\hat{\\beta}\\) é o melhor estimador linear não viesado (BLUE em Inglês):\nEle é linear1 pois \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\) (basta fazer \\(A' = (X'X)^{-1}X'\\)).\nEle é não viesado pois \\(\\mathbb{E}(\\hat{\\beta}) = \\beta\\) e\nEle é o melhor, pois possui a menor variância entre todos os outros estimadores lineares não viesados.\nTeorema\nSeja \\(Y = X \\beta + u\\) com \\(X\\) de posto completo, \\(\\mathbb{E}(u|X) = 0\\) e \\(\\mathbb{V}(u|X) = \\sigma^2 I\\). Então \\(\\hat{\\beta}\\), o estimador MQO de \\(\\beta\\), é o melhor estimador linear não viesado (BLUE) de \\(\\beta\\).\nNote que o Teorema requer que:\nO modelo populacional seja da forma \\(Y = X \\beta + u\\)\n\\(X\\) seja de posto completo (ou seja não existe colineariedade perfeita)\n\\(\\mathbb{E}(u|X) = 0\\)\n\\(\\mathbb{V}(u|X) = \\sigma^2 I\\)\nEssas condições são às vezes conhecidas como as hipóteses de Gauss–Markov. Se alguma das hipóteses de Gauss–Markov não for valida, então \\(\\hat{\\beta}\\) não será mais BLUE.\n\nOu seja Teorema de Gauss-Markov nos diz que se as condições do Teorema forem satisfeitas, não adianta buscar por algum outro estimador linear não viesado, pois \\(\\hat{\\beta}\\) será o melhor (de menor variância).\n\nDemostração\nSeja \\(\\tilde{\\beta}\\) qualquer outro estimador linear não viesado de \\(\\beta\\).\nComo \\(\\tilde{\\beta}\\) é um estimador linear, ele é da forma \\(\\tilde{\\beta} = A'Y\\) (para qualquer matriz \\(A\\) de dimensão \\(n \\times k+1\\) função de \\(X\\).).\nComo \\(\\tilde{\\beta}\\) é não viesado, temos que \\(A'X = I\\) pois \\[\\begin{equation}\n\\begin{aligned}\n\\mathbb{E}(\\tilde{\\beta} | X)  & =  \\mathbb{E}(A'Y | X)\\\\\n                   & =  \\mathbb{E}(A' (X\\beta + u) | X) \\\\\n                   & =  \\mathbb{E}(A'X\\beta|X) + \\mathbb{E}(u|X) \\\\\n                   & =  \\mathbb{E}(A'X\\beta|X) \\quad \\text{pois } \\mathbb{E}(u|X) = 0 \\\\\n                   & =  A'X \\beta, \n\\end{aligned}\n\\end{equation}\\] que é não viesado se e somente se \\(A'X = I\\)\nA variância de \\(\\tilde{\\beta}\\) (condicional em \\(X\\)) é \\[\\begin{equation}\n\\begin{aligned}\n\\mathbb{V}(\\tilde{\\beta}|X) & =  \\mathbb{V}(A'Y|X) \\\\\n                & =  A'\\mathbb{V}(Y|X)A \\\\\n                & =  A'\\mathbb{V}(X \\beta + u|X)A \\\\\n                & =  A'\\mathbb{V}(u|X)A \\\\\n                & =  A'\\sigma^2 I A \\\\\n                & =  \\sigma^2 A'A \\\\\n\\end{aligned}\n\\end{equation}\\]\nDefinamos \\(C = A - X(X'X)^{-1}\\), então \\(X'C = \\underbrace{X'A}_{I}-\\underbrace{X'X(X'X)^{-1}}_{I} = 0\\)\nSabemos que \\(\\mathbb{V}(\\hat{\\beta}|X) = \\sigma^2 (X'X)^{-1}\\). Então basta provar que \\(\\mathbb{V}(\\tilde{\\beta}|X) - \\mathbb{V}(\\hat{\\beta}|X)\\) é semi-definida positiva, ou seja \\[A'A-(X'X)^{-1} \\geq 0.\\] Para provar isto vejamos que \\[\\begin{equation}\n\\begin{aligned}\nA'A-(X'X)^{-1} & =  [C+X(X'X)^{-1}]'[C+X(X'X)^{-1}] -(X'X)^{-1} \\\\\n            & =  [C'+ (X'X)^{-1} X'] [C+X(X'X)^{-1}] -(X'X)^{-1} \\\\\n            & =  C'C +  \\underbrace{C' X}_{0}(X'X)^{-1} + (X'X)^{-1} \\underbrace{X'C}_{0} \\\\ &  + (X'X)^{-1} \\underbrace{X' X(X'X)^{-1}}_{I} -(X'X)^{-1}\\\\\n            & =  C'C + (X'X)^{-1} -(X'X)^{-1} \\\\\n            & =  C'C \\geq 0.\\\\\n\\end{aligned}\n\\end{equation}\\]\nCom isso temos provado que \\(A'A \\geq (X'X)^{-1}\\) ou equivalentemente, \\[\\underbrace{\\sigma^2 A'A}_{\\mathbb{V}(\\tilde{\\beta}|X)} \\geq \\underbrace{\\sigma^2 (X'X)^{-1}}_{\\mathbb{V}(\\hat{\\beta}|X)},\\] que é o que queremos demostrar.\n\\(C'C\\) é de fato semi-definida positiva, veja Teorema A.4 em (Hansen 2020) ou 10.10 em (Seber 2008).\nConclusão\nSob as hipóteses de Gauss-Markov, temos demostrado que o estimador de MQO, \\(\\hat{\\beta}\\), amplamente utilizado em análise de regressão é o melhor estimador linear não viesdado. Isto significa que se as hipóteses de Gauss-Markov são verificadas, não conseguiremos um estimador linear que seja melhor (menor variância) do que \\(\\hat{\\beta}\\).\n\n\n\nHansen, Bruce E. 2020. Econometrics. Online version. Wisconsin.\n\n\nSeber, George AF. 2008. A Matrix Handbook for Statisticians. Vol. 15. John Wiley & Sons.\n\n\nUm estimador linear é um estimador da forma \\(\\tilde{\\beta} = A'Y\\) para uma matriz \\(A\\) de dimensão \\(n \\times k+1\\) função de \\(X\\).↩︎\n",
    "preview": {},
    "last_modified": "2021-06-17T21:32:20-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-25-intro-regressao-linear/",
    "title": "Intro à Regressão Linear",
    "description": "Uma breve introdução à Análise de Regressão Linear: interpretação e implementação no R (e no Python).",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-02-25",
    "categories": [
      "Linear Regression",
      "R",
      "Python"
    ],
    "contents": "\n\nContents\nIntrodução\nEstimação por MQO\nImplementação no R\nInterpretação\nConclusões\nBonus\n\nIntrodução\nUma das técnicas mais conhecidas e difundidas no mundo de statistical/machine learning é a Análise de Regressão Linear (ARL). Ela é útil quando estamos interessados em explicar/predizer a variável dependente \\(y\\) utilizando um conjunto de \\(k\\) variaveis explicativas \\(x_1, \\ldots, x_k\\).\nBasicamente, utilizamos as \\(k\\) variáveis explicativas para entender o comportamento de \\(y\\) e, num contexto de regressão linear, assumimos que a relação entre \\(y\\) e as \\(x\\)’s é dada por uma função linear da forma:\n\\[y = \\underbrace{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k}_{f(x_1, \\ldots, x_k)} + u,\\] em que \\(u\\) é o termo de erro.\nEstimação por MQO\nNa prática, nunca conhecemos \\(\\beta = [\\beta_0, \\beta_1, \\ldots, \\beta_k]'\\) e temos que estima esses valores utilizando os dados. Existem diferentes métodos de estimação, sendo o método de mínimos quadraros ordinários (MQO) um dos mais comumente utilizados1.\nO estimador de MQO é dado por \\[\\hat{\\beta} = (X'X)^{-1}X'Y,\\] e sua respectiva matriz de covariância (condicional em \\(X\\)) é dada por \\[V(\\hat{\\beta}|X) = \\sigma^2(X'X)^{-1}\\] em que \\(Y = [y_1, \\ldots, y_n]'\\) e \\(X = \\begin{bmatrix} 1 & x_{1,1} & \\cdots & x_{1,k} \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_{n,1} & \\cdots & x_{n,k} \\end{bmatrix}\\)\n\\(\\sigma^2\\) nunca é conhecido, então utilizamos \\(\\hat{\\sigma}^2 = \\dfrac{ \\sum_{i=1}^n \\hat{u}_i^2}{n-k-1}\\), que é um estimador não viesado de \\(\\sigma^2\\) (\\(E(\\hat{\\sigma}^2) = \\sigma^2\\)).\nAssim, na prática nós sempre utilizamos \\(\\widehat{V}(\\hat{\\beta}|X) = \\hat{\\sigma}^2(X'X)^{-1}\\) no lugar de \\(V(\\hat{\\beta}|X)\\).\nO desvio padrão, geralmente reportados pelos softwares estatísticos/econométricos, é a raiz quadrada dos elementos na diagonal de \\(\\widehat{V}(\\hat{\\beta}|X)\\).\nO Teorema de Gaus–Markov estabelece que, sob algumas hipóteses (conhecidas como as hipóteses de Gauss-Markov), \\(\\hat{\\beta}\\) é o melhor estimador linear não viesado (BLUE em inglês: Best Linear Unbiased Estimator), ou seja, para qualquer outro estimador linear2 \\(\\tilde{\\beta}\\), \\[V(\\tilde{\\beta}|X) \\geq V(\\hat{\\beta}|X).\\]\nA Figura 1 mostra um exemplo de uma reta de regressão \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) obtida pelo método de MQO.\n\n\n\nFigure 1: OLS regression line example\n\n\n\nImplementação no R\nRealizar uma regressão linear no R não é difícil, para ver como faze-lo utilizaremos o conjunto de dados hprice1 disponível no pacote wooldridge do R.\nSe assumirmos que o modelo populacional é da forma \\[price = \\beta_0 + \\beta_1 bdrms + \\beta_2 lotsize +  \\beta_3 sqrft + \\beta_4 colonial + u,\\] utilizamos o seguintes comandos\n\n\nlibrary(wooldridge)\nmodelo = lm(price~bdrms+lotsize+sqrft+colonial, data = hprice1)\nmodelo\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nCoefficients:\n(Intercept)        bdrms      lotsize        sqrft     colonial  \n -24.126528    11.004292     0.002076     0.124237    13.715542  \n\nO output anterior apenas mostra os \\(\\hat{\\beta}\\)’s, um output mais completo, que inclui o desvio padrão dos \\(\\hat{\\beta}\\)’s, o teste T, teste F, \\(R^2\\) e p-valores pode ser facilmente obtido utilizando a função summary( ).\n\n\nsummary(modelo)\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.268  -38.271   -6.545   28.210  218.040 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.413e+01  2.960e+01  -0.815  0.41741    \nbdrms        1.100e+01  9.515e+00   1.156  0.25080    \nlotsize      2.076e-03  6.427e-04   3.230  0.00177 ** \nsqrft        1.242e-01  1.334e-02   9.314 1.53e-14 ***\ncolonial     1.372e+01  1.464e+01   0.937  0.35146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 59.88 on 83 degrees of freedom\nMultiple R-squared:  0.6758,    Adjusted R-squared:  0.6602 \nF-statistic: 43.25 on 4 and 83 DF,  p-value: < 2.2e-16\n\nInterpretação\nAntes de interpretar os resultados é importante e necessário conhecer os dados e saber quais as unidades de medida das nossas variáveis3.\nPara darmos uma olhada nos dados utilizaremos as funções select( ) e glimpse( ) do pacote dplyr.\n\n\nlibrary(dplyr)\nhprice1 %>% select(price, bdrms, lotsize, sqrft, colonial) %>% glimpse()\n\n\nRows: 88\nColumns: 5\n$ price    <dbl> 300.000, 370.000, 191.000, 195.000, 373.000, 466.27…\n$ bdrms    <int> 4, 3, 3, 3, 4, 5, 3, 3, 3, 3, 4, 5, 3, 3, 3, 4, 4, …\n$ lotsize  <dbl> 6126, 9903, 5200, 4600, 6095, 8566, 9000, 6210, 600…\n$ sqrft    <int> 2438, 2076, 1374, 1448, 2514, 2754, 2067, 1731, 176…\n$ colonial <int> 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, …\n\n\nO pacote dplyr é utilizado para manipulação de dados. A função select( ) seleciona algumas das variaveis contidas em hprice1 e a função glimpse ( ) nos permite ver rapidamente a estrutura dos dados.\nA descrição das variáveis é apresentada a seguir e ela pode ser obtida utilizando diretamente os comandos help(hprice1) ou ?hprice1.\nVariável\nDescrição\nprice\npreço da casa (em milhares de dólares)\nbdrms\nnúmero de quartos\nlotsize\ntamanho do lote da casa (em pés\\(^2\\))\nsqrft\ntamanho da casa (em pés\\(^2\\))\ncolonial\nDummy (=1 se a casa for de estilo colonial)\nConhecendo melhor os dados, vamos então interpretar os resultados:\n\\(\\approx 66\\%\\) da variabilidade do preço (price) é explicada pelo nosso modelo4.\nUtilizando as estatísticas T (\\(H_0: \\beta_i = 0 \\quad \\text{vs.} \\quad H_1: \\beta_i \\neq 0\\)) apenas lotsize e sqrft são estatísticamente significativas (i.e. rejeitamos \\(H_0\\)) ao nível de significância de 5%.\nO incremento em 481 pés\\(^2\\) (pés quadrados) no lote da casa, implica, em média, um incremento de mil dólares5 no preço da casa (permanecendo fixos os outros fatores).\nO incremente em 8 pés\\(^2\\) no tamanho da casa, implica, em édia, um incremento de mil dólares6 no preço da casa (permanecendo fixos os outros fatores).\nFinalmente, summary( ) também fornece informação para testar conjuntamente \\[H_0: \\beta_{bdrms}=0,\\beta_{lotsize}=0,\\beta_{sqrft}=0,\\beta_{colonial}=0\\] vs. \\[H_1: H_0 \\text{ is not true. }\\] Utilizando o teste F, rejeitamos \\(H_0\\) (p-valor \\(\\approx\\) 0, F-statistics = 43.25)\n\nObviamente, nossa interpretação foi realizada assumindo que as hipóteses do modelo linear clássico são satisfeitas. Se as hipóteses não são satisfeitas, precisamos melhoras/corrigir nosso modelo e apenas interpretar os resultados quando as hipóteses do modelo linear clássico forem verificadas.\n\nNo livro do Wooldridge7 encontramos uma interessante discussão sobre como interpretar os \\(\\beta\\)’s quando utilizamos ou não transformações logaritmicas. A seguinte Tabela apresenta um resumo dessa discussão e fornece uma guia para melhor interpretarmos os resultados\nVariável dependente\nVariável independente\nInterpretação do \\(\\beta\\)\n\\(y\\)\n\\(x\\)\n\\(\\Delta y = \\beta \\Delta x\\)\n\\(y\\)\n\\(\\log(x)\\)\n\\(\\Delta y = \\big(\\beta/100 \\big) \\% \\Delta x\\)\n\\(\\log(y)\\)\n\\(x\\)\n\\(\\% \\Delta y = 100\\beta \\Delta x\\)\n\\(\\log(y)\\)\n\\(\\log(x)\\)\n\\(\\% \\Delta y = \\beta \\% \\Delta x\\)\nConclusões\nARL é um métodos estatístico (econométrico, de machine/statiscal learning) poderoso e facil de implementar, ele pode fornecer insights importantes sobre nossos dados (e consequentemente sobre nosso negócio).\nR fornece uma forma facil de utilizar regressão linear e fornece também informação útil para sua interpretação. Contudo, é importante tomar cuidado sobre as hipóteses assumidas no modelo (que é o tópico do nosso próximo post), a não verificação das hipóteses pode ter um forte impacto nos resultados obtidos.\nBonus\nImplementação em Python\n\nimport statsmodels.api as sm\nimport pandas as pd\nfrom patsy import dmatrices\n\nurl = \"https://raw.githubusercontent.com/ctruciosm/statblog/master/datasets/hprice1.csv\"\nhprice1 = pd.read_csv(url)\n\ny, X = dmatrices('price ~ bdrms + lotsize + sqrft + colonial', \n                  data = hprice1, return_type = 'dataframe')\n# Definir o modelo\nmodelo = sm.OLS(y, X)\n# Ajustar (fit) o modelo\nmodelo_fit = modelo.fit()\n# Resultados completos do modelo\nprint(modelo_fit.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.676\nModel:                            OLS   Adj. R-squared:                  0.660\nMethod:                 Least Squares   F-statistic:                     43.25\nDate:                Qui, 17 Jun 2021   Prob (F-statistic):           1.45e-19\nTime:                        21:31:15   Log-Likelihood:                -482.41\nNo. Observations:                  88   AIC:                             974.8\nDf Residuals:                      83   BIC:                             987.2\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -24.1265     29.603     -0.815      0.417     -83.007      34.754\nbdrms         11.0043      9.515      1.156      0.251      -7.921      29.930\nlotsize        0.0021      0.001      3.230      0.002       0.001       0.003\nsqrft          0.1242      0.013      9.314      0.000       0.098       0.151\ncolonial      13.7155     14.637      0.937      0.351     -15.397      42.828\n==============================================================================\nOmnibus:                       24.904   Durbin-Watson:                   2.117\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               45.677\nSkew:                           1.091   Prob(JB):                     1.21e-10\nKurtosis:                       5.774   Cond. No.                     6.43e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.43e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nA ideia básica deste método é encontrar os valores \\(\\hat{\\beta}\\)’s que minimizam a soma de quadrados dos residuos.↩︎\nUm estimador linear é um estimador da forma \\(\\tilde{\\beta} = A'Y\\) em que a matrix \\(A\\) é uma matriz de dimensão \\(n \\times k+1\\) função de \\(X\\)↩︎\nNa prática, antes de ajustar a reta de regressão é feita uma análise exploratória de dados (EDA em inglês). Nessa EDA já conheceremos melhor as variáveis com as que estamos trabalhando, bem como as unidades de medida.↩︎\nGeralmente, preferimos o \\(R^2_{Adjusted}\\) ao \\(R^2\\)↩︎\n\\(2.076e-03*481 = 0.998556 \\approx 1\\)↩︎\n\\(0.1242*8 = 0.9936 \\approx 1\\)↩︎\nWooldridge, J. M. (2016). Introdução à Econometria: Uma abordagem moderna. Cengage.↩︎\n",
    "preview": "posts/2021-02-25-intro-regressao-linear/uma-introduo-regresso-linear_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-17T21:31:17-03:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  }
]
