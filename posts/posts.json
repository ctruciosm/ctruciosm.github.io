[
  {
    "path": "posts/2026-01-30-vs-code-e-ssh-uma-amizade-animal/",
    "title": "VS Code e SSH: uma amizade animal",
    "description": "Como editar código localmente e rodar tudo em servidores remotos de forma simples, rápida e sem dor de cabeça com o Visual Studio Code",
    "author": [
      {
        "name": "Carlos Alan Vieira do Nascimento",
        "url": "http://lattes.cnpq.br/3741205838023196"
      },
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2026-01-30",
    "categories": [
      "VS Code",
      "SSH"
    ],
    "contents": "\n\nContents\nIntrodução\nConfigurando o VS Code\nPré-requisitos\nConfiguração\n\n\nIntrodução\nNo CAREFS, assim como no\nlaboratório virtual do IMECC,\né comum deixarmos simulações de grande porte ou trabalhos empíricos\nrodando por dias.\nUma forma tradicional de fazer isto, é desenvolver tudo na máquina local, enviar o código\npara a nossa unidade H e executar o script remotamente, por exemplo, através da seguinte forma:\nAbrir o terminal, se logar no SSH do IMECC, se logar no SSH do node\n\nssh ctrucios@ssh.ime.unicamp.br\n\n\nssh ctrucios@node06.lv.ime.unicamp.br\n\nRodar o código, com, por exemplo:\n\nR CMD BATCH nome_do_meu_codigo.R &\n\nou\n\nnohup julia nome_do_seu_arquivo.jl outfile_name.txt &\n\nApós um tempo rodando, veremos se nosso protótipo roda sem erro no servidor. No entanto, e se pudéssemos fazer o último teste do código diretamente no ambiente remoto, garantindo que tudo está funcionando exatamente como esperado antes de lançar uma execução longa?\nConfigurando o VS Code\nPré-requisitos\nInstalar o VS Code\nInstalar a extensão Remote – SSH\nConfiguração\nPressione F1 ou fn + F1 (se estiver no Mac) e selecione a opçãoRemote-SSH: Add New SSH Host... (na imagem abaixo aparece como segunda opção).\n\nSelecionar Remote-SSH: Add New SSH Host\nEm seguida, aparecerá uma caixa de diálogo para inserir o endereço SSH da máquina à qual deseja se conectar.\n\nSSH\nNo CAREFS, o acesso é feito conectando-se primeiro via SSH ao IMECC e, em seguida, a um dos nodes. Isso pode ser realizado com o seguinte comando:\n#| eval: false\nssh -J usuario@ssh.ime.unicamp.br usuario@node06.lv.ime.unicamp.br\n\nSSH\n\nSubstitua ctrucios pelo seu nome de usuário e node07 pelo node que deseja utilizar\nApós inserir o comando, será solicitado que você salve a nova configuração.\n\nSalvar configuração\nCom a configuração salva, basta conectar-se ao node desejado:\nPressione F1\nSelecione Remote-SSH: Connect to Host...\nEscolha o host configurado anteriormente\n\nConexão SSH\n(no meu caso, como pode-se ver na imagem, tenho configurado os nodes 6 e 7)\n\nConexão SSH\n(o sistema pedirá sua senha duas vezes, e se for a primeira vez que entra, criará un finger print)\n\nConexão SSH\nDepois, o VS Code abrirá normalmente e você poderá escolher entre abrir um arquivo ou criar um novo (como usual).\n\nVS Code\nPor exemplo, eu abri um arquivo de uma simulação (MonteCarloSimulations.R), aperto Shift + Enter para rodar o código selecionado e pronto…vai dar erro!\n\nVS Code\n\nObservação: o terminal está no bash, é necessário abrir o R primeiro para não dar erro como na imagem anterior.\n\n\nVS Code\nPronto, agora pode editar código no VS Code e rodar tudo no node.\nHappy Coding!\n\n\n\n",
    "preview": {},
    "last_modified": "2026-01-30T10:02:34-03:00",
    "input_file": "vs-code-e-ssh-uma-amizade-animal.knit.md"
  },
  {
    "path": "posts/2025-06-22-paralelizao-no-r-uso-do-future-e-ferramentas-associadas/",
    "title": "Paralelização no R: Uso do _future_ e ferramentas associadas",
    "description": "Este post aborda a paralelização como uma estratégia para reduzir o tempo de execução de scripts, permitindo o uso eficiente de múltiplos núcleos ou máquinas. São apresentadas diferentes formas de distribuição de tarefas, como execução sequencial, sessões independentes e clusters. Além disso, é discutido como identificar gargalos de processamento por meio de medições e testar alternativas para otimizar o código.",
    "author": [
      {
        "name": "João Victor Siqueira Rodrigues",
        "url": "https://jsicas.github.io"
      },
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2025-06-22",
    "categories": [
      "R"
    ],
    "contents": "\n\nContents\nIntrodução\nParalelização\nO pacote future\nO Pacote furrr\nO Pacote doFuture\nO Pacote future.apply\n\nExemplos Adicionais\nExemplo 1: slow_sqrt\nExemplo 2: Números Aleatórios\n\nEficiência de Código\nProfiler\nBenchmark\n\nComentários Finais\n\n\nIntrodução\nÉ bem sabido por estatísticos, econometristas, cientistas e analistas de dados que, tanto R quanto Python não são linguagens particularmente rápidas. Assim, otimizar o desempenho dos códigos torna-se uma parte essencial da rotina de quem trabalha com estas linguagens. Neste post, focaremos na linguagem R e no uso de recursos para paralelização.\n\nPara quem busca desempenho e praticidade, uma linguagem bastante promissora é Julia. Uma excelente iniciativa para começar a aprender essa linguagem é o projeto “Do Zero ao Julia”, que oferece recursos introdutórios em português.\n\nOutro caminho para otimizar o desempenho, além da já famosa vetorização, é através do C++, cuja integração com o R pode ser realizada utilizando o pacote Rcpp.\n\nA computação paralela é uma técnica que divide um grande problema em pequenas tarefas, as quais são resolvidas de forma descentralizada podendo ser processadas em vários núcleos de um computador ou entre computadores, normalmente, sendo mais rápido do que processar as tarefas de forma sequencial (Flinders e Smalley 2024). A computação paralela é resposta a uma busca por maior velocidade de execução dos scripts, motivada pelo interesse em aumentar a eficiência do código em uma era em que modelos cada vez mais complexos são propostos na literatura.\nEste post apresenta as algumas das principais formas de paralelizar funções no R, além de mostrar como identificar gargalos de processamento e testar alternativas para melhorar a eficiência do código.\nParalelização\n\nPor padrão, o R utiliza apenas um núcleo do computador para processar suas tarefas. Paralelização permite a utilização de múltiplos núcleos, diminuindo assim o tempo de processamento.\n\nA Figura 1 apresenta a ideia de como ocorre a paralelização de processos. Note que, ao utilzarmos processamento em paralelo, existe um tempo chamado de overhead que não estava presente no processamento sequencial. Overhead refere-se ao tempo necessário para coordenar e gerenciar tarefas tais como a comunicação entre os núcleos, divisão e agregação de tarefas, gerenciamento de memória, entre outros.\n\n\n\nFigure 1: Conceito da paralelização\n\n\n\nA forma apresentada de paralelização no post abordará 3 dos principais pacotes que utilizam o framework do pacote future, sendo eles os pacotes furrr, doFuture e future.apply. É importante mencionar que, além dos pacotes aqui apresentados, existem outros que também podem ser utilizados.\nO pacote future\n\n\nO pacote future, parte do ecossistema futureverse, é um framework que unifica e simplifica a paralelização no R. Para isso, ele apresenta uma abstração do conceito de atribuição, permitindo o cálculo de valores de forma assíncrona (futura) e resolvidos via futures. Existem 4 planos para resolver um future:\nsequential: resolve futures sequencialmente na sessão em utilização do R, sendo a forma padrão de funcionamento do R;\nmultisession: resolve futures de forma assíncrona e paralelizada gerando, em segundo plano, novas sessões independentes do R que se comunicam com o processo principal;\nmulticore: resolve futures de forma assíncrona e paralelizada gerando forked R processes que rodam em segundo plano. Não suportado em sistemas Windows;\ncluster: resolve futures de forma assíncrona e paralelizada em sessões independentes do R, normalmente utilizado para duas ou mais máquinas.\nAdemais, além de selecionar o plano, também é necessário escolher a quantidade de núcleos (workers) a serem utilizados, configurações definidas por meio da função plan().\n\nPor exemplo, o comando plan(multisession, workers = 3) configura o plano como multisession e define a utilização de 3 workers.\n\nAlém da função plan(), as seguintes funções são bastante úteis quando o interesse é paralelizar\nplan(): define a estratégia a ser utilizada, além de, ao ser utilizada sem argumentos, retorna a estratégia atual;\navailableCores(): retorna a quantidade de núcleos que a máquina tem;\nnbrOfWorkers(): retorna a quantidade de workers definidos no comando plan();\nnbrOfFreeWorkers(): retorna a quantidade de workers disponíveis, isto é, os que não estão executando nenhuma tarefa.\nPor exemplo, a seguir são apresentadas as configurações sem alteração do número de workers, de um dos autores no momento de escrever este post:\n\n\nplan()\n\nsequential:\n- args: function (..., envir = parent.frame(), workers = \"<NULL>\")\n- tweaked: FALSE\n- call: NULL\n\navailableCores()\n\nsystem \n    12 \n\nnbrOfWorkers()\n\n[1] 1\n\nnbrOfFreeWorkers()\n\n[1] 1\n\nO Pacote furrr\n\n\nO pacote furrr busca combinar os comandos e ferramentas de programação funcional do pacote purrr com o processamento em paralelo do future. Essa integração possibilita a execução paralela de funções de mapeamento com poucas alterações no código, sendo necessário acrescentar o prefixo future_ às funções do purrr. Por exemplo, a versão paralela da função map() é a future_map(), a versão paralela da função imap() é a future_imap(), e assim por diante.\n\nA seguir, apresentamos um exemplo para ilustrar como utilizar a função future_map_dbl() para aplicar a função log() nos elementos de um vetor utilizando 2 workers:\n\n\nlibrary(furrr)\nplan(multisession, workers = 2)  # paralelizar utilizando 2 núcleos\n\nvet <- c(2, exp(1), 10, 20)  # vetor para teste\nfuture_map_dbl(vet, log)\n\n[1] 0.6931472 1.0000000 2.3025851 2.9957323\n\nNa prática, é comum o uso de funções mais complexas, as quais poderão ser incluidas através de duas formas: (a) por meio de funções anônimas, criadas a partir de function(x) {...} (ou \\(x) {...}, para versões superiores ao R 4.1.0) e (b) por medio de uma forma exclusiva do furrr. Para ilustrar isto, veja o exemplo a seguir:\n\n\n# forma 1\nf1 <- future_map(vet, \\(i) {\n  c('log' = log(i), 'quadrado' = i^2)\n})\n\n# forma 2 (exclusiva do furrr)\nf2 <- future_map(vet, ~ c('log' = log(.x), 'quadrado' = .x^2))\n\n# resultado\nidentical(f1, f2)  # TRUE\n\n[1] TRUE\n\nf1  \n\n[[1]]\n      log  quadrado \n0.6931472 4.0000000 \n\n[[2]]\n     log quadrado \n1.000000 7.389056 \n\n[[3]]\n       log   quadrado \n  2.302585 100.000000 \n\n[[4]]\n       log   quadrado \n  2.995732 400.000000 \n\nO Pacote doFuture\nO pacote doFuture é outra alternativa para realizar a paralelização de processos. Ele pode ser utilizado de duas formas: a primeira, mais recomendada, é através do operador %dofuture%, enquanto a segunda, menos recomendada, é através do operador %dopar%.\n\nA seguir, será utilizado o pacote doFuture para aplicar a função \\(log()\\) nos elementos de um vetor utilizando 2 núcleos e retornar um vetor como saída.\n\n\nlibrary(doFuture)\nplan(multisession, workers = 2)  # paralelizar utilizando 2 núcleos\n\nvet <- c(2, exp(1), 10, 20)  # vetor para teste\nforeach(i = vet, .combine = c) %dofuture% {\n  log(i)\n}\n\n[1] 0.6931472 1.0000000 2.3025851 2.9957323\n\nVale ressaltar que a saída padrão do foreach() é em formato de lista. Contudo, é possível configurá-la por meio do argumento .combine, que neste caso foi ajustada para ter um vetor como resultado. Além disso, destaca-se que o foreach() NÃO é um um for-loop, para mais detalhes, veja (Bengtsson 2022, Apêndice 8.8).\nO Pacote future.apply\nO pacote future.apply estende as funções da base do R que aplicam operações à vetores, matrizes e listas, permitindo a execução de forma paralela com o suporte do framework do future. Este pacote fornece versões paralelizáveis de funções da família apply() presente na base do R, bastando adicionar o prefixo future_.\n\nA seguir, está apresentado como utilizar o pacote future.apply para aplicar a função \\(log()\\) nos elementos de um vetor utilizando 2 núcleos:\n\n\nlibrary(future.apply)\nplan(multisession, workers = 2)  # paralelizar utilizando 2 núcleos\n\nvet <- c(2, exp(1), 10, 20)  # vetor para teste\nfuture_sapply(vet, log)\n\n[1] 0.6931472 1.0000000 2.3025851 2.9957323\n\nAlém do future_sapply(), temos também o future_apply() e o future_lapply().\nExemplos Adicionais\nNesta section dois exemplos adicionais serão abordados, o primeiro, relativo à comparação do tempo de execução com e sem paralelização considerando os pacotes apresentados, enquanto o segundo discutirá brevemente a geração de números aleatórios e cuidados necessários ao utilizar em conjunto com a computação paralela.\nExemplo 1: slow_sqrt\nSerá aplicado a função slow_sqrt(), a qual espera 1 segundo e calcula a raiz quadrada do valor de entrada. Além disso, será apresentado o tempo de execução para aplicar essa função em cada elemento de um vetor de tamanho 60 com e sem paralelização.\nPrimeiro será feito o setup necessário:\n\n\nlibrary(future)\n\nplan(multisession, workers = 5)  # paralelizar utilizando 5 núcleos\n# função slow_sqrt\nslow_sqrt <- function(x) {\n  Sys.sleep(1)  # simula 1 segundo de processamento\n  sqrt(x)\n}\n\n# execução sem paralelização\nvet <- 1:60  # vetor para teste\n\n\nA seguir, a comparação utilizando processamento sequencual e as versões em paralelo\n\n\nmicrobenchmark(M1 = lapply(vet, slow_sqrt), \n               M2 = future_map(vet, slow_sqrt),\n               M3 = foreach(i = vet) %dofuture% {\n                      slow_sqrt(i)\n                    },\n               M4 = future_lapply(vet, slow_sqrt), times = 10)\n\nUnit: seconds\n expr      min       lq     mean   median       uq      max neval\n   M1 59.25603 59.26126 59.31608 59.26598 59.40455 59.48470    10\n   M2 12.01358 12.01692 12.15005 12.02559 12.05385 13.23094    10\n   M3 12.00944 12.02402 12.03647 12.02777 12.06402 12.06695    10\n   M4 12.00339 12.01313 12.02976 12.01728 12.05871 12.06682    10\n\nExemplo 2: Números Aleatórios\nConsidere as variáveis aleatórias \\(X_1 \\sim \\mathcal{N}(0,1)\\), \\(X_2 \\sim \\mathcal{N}(10, 5)\\) e \\(X_3 \\sim \\mathcal{N}(50, 8)\\). Deverá ser gerado uma amostra com 1000 observações para cada variável aleatória apresentada e calculado o mínimo, média, variância e máximo utilizando \\(4\\) núcleos considerando a seed 123.\nPrimeiro será feito o setup necessário:\n\n\nplan(multisession, workers = 4)  # paralelizar utilizando 4 núcleos\n\n# execução sem paralelização\nmu <- c(0, 10, 50)\nsigma <- c(1, 5, 8)\n\nset.seed(123)\nlapply(1:3, \\(i) {\n  amostra <- rnorm(1000, mu[i], sigma[i])  # gerando amostra\n  c('min'=min(amostra), 'media'=mean(amostra), 'var'=var(amostra), 'max'=max(amostra))\n})\n\n[[1]]\n        min       media         var         max \n-2.80977468  0.01612787  0.98345893  3.24103993 \n\n[[2]]\n      min     media       var       max \n-5.239304 10.212326 25.486049 26.951854 \n\n[[3]]\n     min    media      var      max \n27.21163 49.83910 61.25973 77.36876 \n\n\n\nfurrr\n\n\n# forma 1\nset.seed(123)\nf1 <- future_map(1:3, \\(i) {\n  amostra <- rnorm(1000, mu[i], sigma[i])  # gerando amostra\n  c('min'=min(amostra), 'media'=mean(amostra), 'var'=var(amostra), 'max'=max(amostra))\n}, .options = furrr_options(seed = TRUE))\n\n# forma 2 (exclusiva do furrr)\nset.seed(123)\nf2 <- future_map(1:3, ~ {\n  amostra <- rnorm(1000, mu[.x], sigma[.x])  # gerando amostra\n  c('min'=min(amostra), 'media'=mean(amostra), 'var'=var(amostra), 'max'=max(amostra))\n}, .options = furrr_options(seed = TRUE))\n\n# resultado\nidentical(f1, f2)\n\n[1] TRUE\n\nf1\n\n[[1]]\n        min       media         var         max \n-3.06732143 -0.01880173  1.10208379  3.40602221 \n\n[[2]]\n      min     media       var       max \n-4.215841 10.052563 25.919922 26.973502 \n\n[[3]]\n     min    media      var      max \n26.47221 50.12970 62.78559 73.13110 \n\n\n\ndoFuture\n\n\nset.seed(123)\nforeach(i = 1:3, .options.future = list(seed = TRUE)) %dofuture% {\n  amostra <- rnorm(1000, mu[i], sigma[i])  # gerando amostra\n  c('min'=min(amostra), 'media'=mean(amostra), 'var'=var(amostra), 'max'=max(amostra))\n}\n\n[[1]]\n        min       media         var         max \n-3.06732143 -0.01880173  1.10208379  3.40602221 \n\n[[2]]\n      min     media       var       max \n-4.215841 10.052563 25.919922 26.973502 \n\n[[3]]\n     min    media      var      max \n26.47221 50.12970 62.78559 73.13110 \n\n\n\nfuture.apply\n\n\nset.seed(123)\nfuture_lapply(1:3, \\(i) {\n  amostra <- rnorm(1000, mu[i], sigma[i])  # gerando amostra\n  c('min'=min(amostra), 'media'=mean(amostra), 'var'=var(amostra), 'max'=max(amostra))\n}, future.seed = TRUE)\n\n[[1]]\n        min       media         var         max \n-3.06732143 -0.01880173  1.10208379  3.40602221 \n\n[[2]]\n      min     media       var       max \n-4.215841 10.052563 25.919922 26.973502 \n\n[[3]]\n     min    media      var      max \n26.47221 50.12970 62.78559 73.13110 \n\n\n\nVeja que o resultado utilizando a paralelização diverge do que foi encontrado ao não utilizá-la. Além disso, note que há um argumento referende à seed em cada função dos pacotes apresentados, por exemplo, para o pacote furrr é utilizado o argumento .options = furrr_options(seed = TRUE). Estes argumentos são responsáveis por manter a reprodutibilidade na geração de números pseudo-aleatórios através do gerador L’Ecuyer-CMRG (utilizado pelo future), já que o Mersenne-Twister (gerador padrão do R) não foi idealizado para trabalhar com paralelização, podendo gerar números correlacionados entre os workers, causando viés e conclusões equivocadas.\nAssim, como os geradores são diferentes, o resultado não é o mesmo, contudo, ambos são válidos, isto é, todos são amostras das respectivas normais apresentadas.\nEficiência de Código\nA busca por soluções que diminuem o tempo de processamento computacional (como a computação paralela) vem acompanhadas da preocupação com a sua eficiência. Isto, pois ao se escrever um código mais eficiente, o tempo de execução tende a diminuir. No entanto, nem sempre vale a pena investir tempo na otimização prematura do código. Antes disso, é fundamental avaliar cuidadosamente se esse esforço é realmente necessário (Baldauf 2023):\nQuanto tempo é salvo pela otimização?\nQual a frequência que este código é executado?\nQuanto tempo será necessários para otimizar o código?\nAassumindo que é necessária a otimização no código, torna-se importante detectar quais as linhas que mais consomem tempo. Para isso, ferramentas bastantes úteis são os pacotes profvis para detecção do gargalo no processamento e microbenchmark para testar tempo de processamento de diversas alternativas, como realizado no exemplo 1.\nImportante:  algumas boas práticas que podem ajudar a reduzir o tempo de processamento (Baldauf 2023):\nQuando necessário, armazenar resultados intermediários constatemente utilizados para evitar o recálculo;\nPreferir vetorizar o código e substituir for-loops por funções vetorizadas;\nNão paralelizar processos muito simples devido ao overhead;\nEvitar manipular objetos grandes na paralelização.\nProfiler\nPara identificar as partes críticas do código, isto é, o gargalo do processamento pode-se utilizar o pacote profvis, o qual examina o tempo gasto em cada função de determinada parte do script. Seu acesso se dá por Profile > Start Profiling (e Profile > Stop Profiling para finalizar) na barra de ferramentas do Rstudio ou através da função profvis().\n\nVeja um exemplo abaixo (adaptado de Wickham et al. (2024)):\n\n\n\n\n\n\n\n\n\n\n\nlibrary(profvis)\n\n# funções\nh <- function() {\n  pause(0.1)\n}\n\ng <- function() {\n  pause(0.1)\n  h()\n}\n\nf <- function() {\n  pause(0.1)\n  g()\n  h()\n}\n\n# profiler\nprofvis({\n  f()\n  g()\n})\n\n\n\n\nSua análise está dividida em duas abas, a primeira, flame graph, é apresentado o tempo e memória gastos em cada linha de código em conjunto com uma timeline das funções utilizadas e, a segunda, Data, apresenta o tempo gasto em cada função.\nBenchmark\nPor último, o mesmo código pode ser escrito de várias formas diferentes, sendo necessário determinar qual delas é a mais rápida. Para isso existe o pacote microbenchmark, o qual auxilia na comparação entre as versões do mesmo código de forma sistematizada.\nConsidere comparar 3 funções diferentes cujo objetivo é calcular a soma de um vetor aplicando o logarítimo em cada elemento. A primeira, utiliza funções vetorizadas, a segunda e a terceira aplicam os comandos elemento à elemento, contudo, uma já aloca a memória necessária criando o vetor x_log do seu tamanho final, o que é mais eficiente.\n\n\nlibrary(microbenchmark)\n\n# funções\nf1 <- function(x) {  # utilizando funções vetorizadas\n  x_log <- log(x)\n  sum(x_log)\n}\n\nf2 <- function(x) {  # criando vetor do tamanho final (pré-alocando memória)\n  x_log <- vector(mode = 'numeric', length = length(x))  # tamanho final\n  for (i in 1:length(x)) {\n    x_log[i] <- log(x[i])\n  }\n  sum(x_log)\n}\n\nf3 <- function(x) {  # utilizando o comando append() (ineficiente)\n  x_log <- vector(mode = 'numeric', length = 0)  # tamanho 0\n  for (i in x) {\n    x_log <- append(x_log, log(i))\n  }\n  sum(x_log)\n}\n\n# vetor para teste \nvet <- sample(1:10, size = 3500, replace = TRUE)\n\n# benchmark\ncomp <- microbenchmark(\n  'forma 1' = f1(vet),\n  'forma 2' = f2(vet),\n  'forma 3' = f3(vet)\n)\n\n# resultado\ncomp  \n\nUnit: microseconds\n    expr       min        lq        mean     median         uq\n forma 1    23.452    25.051    36.90041    25.6660    27.3265\n forma 2   177.325   182.081   210.01061   184.3975   187.7800\n forma 3 15469.874 18693.376 19634.10378 19021.0890 19515.1390\n       max neval\n  1087.443   100\n  2686.484   100\n 76993.982   100\n\n# plot do resultado\nlibrary(ggplot2)\nautoplot(comp)\n\n\n\nEntão, veja que a forma 1, isto é, a função f1(), é a mais eficiente (resultado esperado, afinal funções vetorizadas no R são mais eficientes que for-loops). Além disso, veja que a forma 2 se sobressai referente à forma 3, haja vista que o vetor para armazenar o resultado x_log já é criado com o seu tamanho final, enquanto que a forma 3 fica acrescentando ao final do vetor (na verdade o R faz uma cópia do objeto e faz o acréscimo do novo valor, logo, quanto maior for o objeto pior será o desempenho dessa proposta).\nComentários Finais\nExiste a alternativa de executar código como um background job. Para fazer isso no Rstudio basta buscar a aba de background job (Figura 2), selecionar o script e o diretório de trabalho e iniciar. Com isso, o script será executado em uma nova sessão, deixando o terminal disponível para utilização mesmo durante sua execução.\n\n\n\nFigure 2: Background jobs no Rstudio\n\n\n\nUma estratégia interessante é escrever parte do código em C++ (através do pacote Rcpp) e paralelizar com, por exemplo, future_apply() (que é uma das formas mais fáceis aqui apresentadas). Infelizmente, as tentativas previas realizadas por um dos autores não foram satisfatórias. Uma solução foi utilizar o função mclapply() do pacote parallel.\nA maior parte deste post foi escrita pelo primeiro autor, João Victor Siqueira Rodrigues, a pedido do segundo autor. Este último realizou pequenas modificações no texto original e acrescentou pequenos comentários. O Segundo autor agradece a João Victor pela disponibilidade e colaboração na elaboração de um conteúdo que, certamente, será útil para seus colegas.\nHappy Coding!\n\n\n\nBaldauf, Selina. 2023. Efficient R Scientific workflows: Tools and Tips. GitHub Pages. https://selinazitrone.github.io/tools_and_tips/slides/2023_11_16_efficient_r.html#/title-slide.\n\n\nBengtsson, Henrik. 2022. Future Framework: Parallel and Distributed Processing in R. GitHub Pages. https://henrikbengtsson.github.io/future-tutorial-user2022/appendix.html#appendix-foreach-is-not-a-for-loop.\n\n\nFlinders, Mesh, e Ian Smalley. 2024. What is parallel computing? IBM. https://www.ibm.com/think/topics/parallel-computing.\n\n\nWickham, Hadley, Winston Chang, Javier Luraschi, e Timothy Mastny. 2024. profvis: Interactive Visualizations for Profiling R Code. https://profvis.r-lib.org.\n\n\n\n\n",
    "preview": "posts/2025-06-22-paralelizao-no-r-uso-do-future-e-ferramentas-associadas/figuras/conceito_paralelizacao.png",
    "last_modified": "2025-06-22T18:37:36-03:00",
    "input_file": {},
    "preview_width": 1787,
    "preview_height": 1050
  },
  {
    "path": "posts/2025-06-05-economatica/",
    "title": "Economatica",
    "description": "Como acessar ao Economatica no IMECC?",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2025-06-05",
    "categories": [
      "Datasets"
    ],
    "contents": "\n\nContents\nIntrodução\nComputadores liberados\nAcesso\nBoas práticas\n\nIntrodução\nComo muitos de vocês sabem, trabalho com séries temporais financeiras, com ênfase em alocação de carteiras e na modelagem e previsão de medidas de risco (volatilidade, VaR, ES, etc), nos contextos univariado, multivariado e de alta dimensão.\nDurante anos, utilizei dados do Yahoo Finance em minhas pesquisas. No entanto, recentemente percebi que esses dados deixaram de ser confiáveis. Em particular, encontrei casos em que os preços de abertura ou fechamento estavam fora do intervalo formado pelos preços mínimo e máximo (o que obviamente, não faz sentido nenhum).\nMotivado por essa limitação, pela necessidade de dados confiáveis para o desenvolvimento dos meus projetos de pesquisa (obrigado à FAPESP: 2022/09122-0 e ao FAEPEX: PIND 2525/23 pelo suporte financeiro) e também pela alta demanda de alunos e professores nas áreas de séries temporais, econometria e finanças, iniciei, junto aos professores Mauricio Zevallos e Luiz Hotta, com apoio do IMECC e do BIMECC, um pedido de aquisição de licenças do Economatica para o IMECC.\nO pedido foi aprovado, e desde outubro de 2024, a comunidade do IMECC passou a ter acesso a essa base de dados profissional e confiável.\nComputadores liberados\nOs acessos são liberados por IP e atualmente os computadores disnponíveis no Laboratório de Séries Temporais, Econometria e Finanças do IMECC (CAREFS), bem como algusn computadores da Biblioteca do IMECC tem os IPs liberados e os usuários podem utilizar os dados nas suas pesquisas, trabalhos acadêmicos ou mesmo apenas por curiosidade.\nAcesso\nO acesso à plataforma Economatica é bastante simples. A seguir, apresento um passo a passo detalhado:\nRequisito essencial: você deve utilizar um computador com IP autorizado.\nPara os usuários do CAREFS, qualquer computador com conexão cabeada à internet no laboratório já atende a esse requisito.\nPara quem for acessar a partir da biblioteca, é necessário consultar quais são os computadores liberados para uso.\n\nAcesse o site www.economatica.com/. A seguinte tela será exibida:\n\n\n\n\n\nClique em “LOGIN”. Em seguida, aparecerá esta tela:\n\n\n\n\n\nSelecione a opção “Plataforma” e enseguida aparecerá uma opção para fazer login. Abaixo apresento duas possíveis telas que podem aparecer quando clicar em “login”\n\n\nTela errada\nTela certa\n\nSe a tela que aparecer for semelhante à “Tela errada”, significa que o IP do computador não está liberado para uso. A “Tela certa” deve só pedir email e você pode colocar seu email institucional.\nApós alguns segundos (ou até um par de minutos de carregamento), a janela principal da ferramenta será aberta:\n\n\n\n\n\n\nDepois disso, é só aproveitar todos os benfícios que Economatica pode oferecer. No canal oficial no YouTube da Economatica, há diversos tutoriais disponíveis para te ajudar a aprender a utilizar a plataforma da melhor forma possível..\n\nEnjoy!\nBoas práticas\nTente salvar os arquivos seguindo o padrão: username_mercado_tipo_frequencia. Por exemplo, se você estiver montando uma base com os retornos diários das ações que compõem o índice IBRx, o nome do arquivo poderia ser ctrucios_ibrx_retornos_diarios (dessa forma, outros usuários buscando os mesmos dados poderão utilizar)\nO arquivo, assim que for aberto, atualizará a base com os dados mais recentes, bastando apenas salvar o arquivo no computador.\nSe for utilizar um arquivo salvo de outro usuário, apenas abra o arquivo e baixe os dados (mas não altere nenhum parâmetro na construção na base)!.\n\n\n\n",
    "preview": "posts/2025-06-05-economatica/economatica_01.png",
    "last_modified": "2025-06-06T14:32:22-03:00",
    "input_file": {},
    "preview_width": 2722,
    "preview_height": 974
  },
  {
    "path": "posts/2025-02-17-book-review-hands-on-time-series-analysis-with-r/",
    "title": "Book Review: Hands-On Time Series Analysis with R",
    "description": "Minhas impressões do livro \"Hands-On Time Series Analysis with R\" do Rami Krispin.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2025-02-17",
    "categories": [
      "Book Review",
      "R",
      "rstats"
    ],
    "contents": "\n\nContents\nOverview do livro\nComentários gerais\nPrós\nContras\n\nConclusão\nBônus\n\n\nOverview do livro\n\n\n\n\n\nNo 2025.1 ministrarei Séries Temporais para os alunos da graduação em Estatística do IMECC/UNICAMP de novo. É a segunda vez que ministrarei a disciplina e, tentando trazer um novo olhar para ela, resolvi ler um livro mais mão na massa.\nO autor do livro é bastante ativo no linkedIn, o que me motivou a ler o livro que, por sinal, é editado pela Packt, uma editora que tem vários livros do estilo mão na massa.\nComentários gerais\nO livro conta com 12 capítulos curtos, o que facilita bastante o acompanhamento da leitura. Além disso, os códigos fornecidos ao longo do conteúdo, juntamente com os comentários nas análises exploratórias, são extremamente úteis e valiosos para quem trabalha com séries temporais no cotidiano.\nAo contrário da revisão de outro livro prático que fiz há algum tempo, gostei muito mais da leitura de Hands-On Time Series Analysis with R do @krispin2019. Embora o livro apresente alguns pequenos erros de digitação, eles são facilmente identificáveis e não comprometem nem a leitura nem a compreensão dos conceitos abordados.\nPrós\nO livro disponibiliza os códigos em um repositório público, o que facilita ainda mais o aprendizado prático.\nEstá repleto de comentários valiosos, que refletem a experiência de quem realmente trabalha na área e sabe utilizar o R de maneira eficaz.\nOs capítulos são curtos e objetivos, o que facilita terminar a leitura sem interromper o fluxo, perfeito para concluir “antes de dormir”.\nO autor aborda temas abrangentes que, com exceção do último capítulo, estão alinhados com a ementa de um curso de séries temporais de nível graduação.\nUma das boas surpresas do livro é a introdução ao pacote h2o, uma excelente ferramenta para trabalhar com aprendizado de máquina, além de exemplos práticos de como aplicá-lo a séries temporais.\nOs primeiros 10 capítulos são muito bons, abordando os tópicos de maneira direta e com comentários práticos que realmente agregam ao aprendizado.\nContras\nO livro utiliza exclusivamente códigos em R (e eu sou fã do R, mas reconheço que muitos “Pythonicos” podem sentir falta de exemplos em Python).\nA diferença dos 10 primeiros capítulos, eu não gostei muito do capítulo 11 (ARIMA). Ele não é ruim, mas achei prático demais para meu gosto. Por exemplo, ele não discute o possível problema de cancelamento de raizes em modelo ARMA, um tema que eu considero importante.\nO último capítulo, que trata de aprendizado de máquina, é mais um case prático. O autor deixa claro que o objetivo não é aprofundar-se nos métodos de aprendizado de máquina, mas sim demonstrar como são aplicados em séries temporais. No entanto, senti falta de mais detalhes, especialmente porque esse é um tema pouco explorado em livros-texto sobre séries temporais.\nConclusão\nDe maneira geral, gostei bastante do livro e acabei utilizando vários dos códigos e comentários presentes nele nas minhas próprias notas de aula. Inclusive, decidi incluí-lo como referência bibliográfica complementar na ementa da disciplina.\nCaso queira saber mais acerca do curso que ministrarei, pode ver a informação aqui.\nBônus\nO Canal Data Scientist Show fez uma entrevista de 1.5 horas ao Rami Krispin (autor do livro), segue o link para os interessados: youtube.com/watch?v=eHSJA_lej4c.\n\n\n\n",
    "preview": "posts/2025-02-17-book-review-hands-on-time-series-analysis-with-r/hotsawr.png",
    "last_modified": "2025-02-18T11:15:41-03:00",
    "input_file": {},
    "preview_width": 810,
    "preview_height": 1000
  },
  {
    "path": "posts/2024-01-28-como-chegar-do-aeroporto-de-guarulhos-at-a-unicamp/",
    "title": "Do GRU até a UNICAMP",
    "description": "Um passo a passo de como chegar do aeroporto de Cumbica, em Guarulhos (GRU) até a Universidade Estadual de Campinas (UNICAMP) em Campinas.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2024-01-28",
    "categories": [
      "UNICAMP"
    ],
    "contents": "\n\nContents\nChegando no aeroporto\nDo aeroporto até Campinas\nDe Campinas até a UNICAMP\nPreços\n\nChegando no aeroporto\n\n\n\n\n\nAo chegar no aeroporto de Guarulhos (GRU) você chegara no Terminal 2\nou no Terminal 3. Viagens nacionais chegam no Terminal 2 enquanto que\nviagens internacionais chegam, geralmente1, no\nTerminal 3.\nÉ possível se deslocar de um terminal para outro por dentro do\naeroporto (7 min a pé), mas também existem ônibus do proprio aeroporto\nque fazem esse percurso e são gratuitos.\nDo aeroporto até Campinas\n\n\n\n\n\nExistem várias formas de chegar até Campinas, aqui explico algumas\ndelas:\nNo Terminal 2 tem um ônibus da Lira, que vai direto para a\nRodoviária de Campinas. Para comprar online basta entrar no site da Lira e colocar:\nOrigem: Aeroporto de Cumbica.\nDestino: Campinas. Existe também um guiché no próprio Terminal 2 (do\nlado de fora). Pergunte aos seguranças onde que fica o guiché do Lira.\nQuando comprar suas passagem, não esqueça perguntar onde que pega o\nônibus.\n\nNo Terminal 2 ou 3, pegue um Uber até o Terminal Rodoviário Tiete\n(Rodoviária de São Paulo) e de lá, pegue um ônibus até a Rodoviária de\nCampinas. Se você fora o campus de Limeira, pode pegar um ônibus, também\nda Lira, com destino Limeira.\nPegue o ônibus\ndo próprio aeroporto (aquele que é gratuito) que vá até o Terminal\n1, daí pegue o trêm\naté a estação Luz e na estação Luz mude para o metro linha azul com\ndestino à estação Portuguesa-Tiete (que é no Terminal Rodoviário Tiete),\nnão precisa comprar outra passagem, basta fazer baldeação. Esta é a\nopção mais barata, mas não recomendo para quem vem com mala.\nSe for pegar o trêm, fique atento aos horários, pois o trêm passa de\nhora em hora. Para os interessados, tem esse video aqui\nque mostra como ir do aeroporto de Guarulhos até o centro de são\nPaulo.\n\nObservação: o trêm expresso aeroporto (que vai do\nGRU até a estação Luz) vá até o Terminal Rodoviário Barra funda, mas\neste terminal não é o Terminal Tiete, são dois terminais diferentes.\n\nDe Campinas até a UNICAMP\n\n\n\n\n\nChegando da rodoviária de Campinas, tem duas formas de chegar até a\nUNICAMP: 1. Uber (ou algum outro transporte por aplicativo): Taxis em\nCampinas são bem mais caros do que transporte por aplicativo, então se\ntiver internet, um Uber pode ser a melhor opção. Já se não tiver\ninternet, na saida da Rodoviária (no andar de cima) tem vários taxis\nesperando. 2. Ônibus: pode pegar o 332 que vá até o Hospital das\nClinicas (dentro da Universidade) e daí andar ou pegar um Uber até seu\ndestino. Pode pegar também o 331 que vá até o Terminal de Barão Geraldo\n(Distrito onde fica a UNICAMP).\n\nObservação: Para pegar ônibus em Campinas, precisará\nprimeiro criar seu cadastro para poder utilizar seu bilhete\nÚnico no celular e também precisará colocar crédito no bilhete\nÚnico. Isto apenas é possível se tiver CPF.\n\nPreços\nOs valores aqui são aproximados e em Real:\nServiço\nValor\nUber: Aeroporto >>> Rodoviária\nTiete\nR. $70-$80\nÔnibus: Aeroporto >>> Rodoviária\nde Campinas\nR. $ 47.75\nÔnibus: Rod. Tiete >>> Rod. de\nCampinas\nR. $ 45\nUber: Rodoviária de Campinas >>>\nUNICAMP\nR. $35 - $50\nTrem: Terminal 1 >>> Rod.\nTiete\nR. $5\nLembre-se que o Uber tem preço dinâmico, então, dependendo do horário\nqu você chegar pode pegar bem mais. Eu já paguei R.$60, mas também já\ntive que pagar R.$150 um dia que cheguei de madrugada.\nA passagem de trêm você compra na mesma estação e só precisa comprar\numa passagem, o preço é aprox. 5 reais (mas não sei exatamente o\nvalor).\nSeja que você esteja vindo para fazer graduação, intercâmbio,\nmestrado, doutorado, pós-doutorado, ou mesmo ser professor. Seja\nbem-vindo(a) a uma das melhores universidades do Brasil, da América\nLatina e do mundo.\n\nAlgumas viagens internacionais ainda chegam no Terminal\n2↩︎\n",
    "preview": "posts/2024-01-28-como-chegar-do-aeroporto-de-guarulhos-at-a-unicamp/gru-airport.jpg",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-28-book-review-practical-time-series-analysis-prediction-with-statistics-and-machine-learning/",
    "title": "Book Review: Practical Time Series Analysis--Prediction with Statistics and Machine Learning",
    "description": "Minhas impressões do livro \"Practical Time Series Analysis--Prediction with Statistics and Machine Learning\" da Aileen Nielsen",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2022-12-30",
    "categories": [
      "Book Review",
      "R",
      "rstats",
      "Python"
    ],
    "contents": "\n\nContents\nOverview do livro\nComentários gerais\nPrós\nContras\n\nConclusão\n\nOverview do livro\n\n\n\n\n\nNo 2023.1 ministraterei Séries Temporais para os\nalunos de graduação em Estatística do IMECC/UNICAMP. Assim,\npensando em como preparar um material atualizado e atraente para os\nalunos, resolvi ler algumas referências bibliográficas diferentes às que\njá estou acostumado, referências não tão tradicionais, começando por\nNielsen (2019).\nEscolhi começar com Nielsen (2019)\npor ser um livro moderno, popular entre analistas e cientistas de dados,\nter uma versão em português e disponibilizar os códigos do proprio livro\nneste\nrepositório.\nComentários gerais\nO livro tem 17 capítulos, a leitura é fácil e rápida (a versão em\nportugues tem alguns paragrafos confussos, mas pode ser coisa da\ntradução). Embora a tabela de conteudo do livro seja bem abrangente, em\ngeral eu não gostei muito do livro, mas tem alguns pontos positivos\ninteressante que descrevo a seguir.\nPrós\nO livro fornece os códigos em um repositório\npúblico, o que torna o aprendizado mais fácil.\nTodos os capítulos tem referências para leituras mais avançadas de\nforma que o leitor pode se aprofundar o quanto quiser em cada um dos\ntópicos.\nA autora utiliza ambos R e Python,\ntornamdo-se em uma alternativa interessante para todo tipo de\nusuário.\nO capítulo 1 é um dos meus favoritos, você fica bem motivado e com\nvontade de ler os outros capítulos do livro, bem como aprender mais de\nséries temporais.\nO capítulo 10 aborda redes neurais, um tópico pouco abordado por\nlivros mais clássicos (embora a teoria que suporte o uso de redes\nneurais em séries temporais seja bem insipiente ainda), vale a pena a\nleitura.\nO capítulo 11 discute como avaliar a performance do modelo. Embora\nseja apresentado de forma introdutoria, achei interessante dedicar um\ncapítulo para isso. Além do mais, o leitor interessado pode se\naprofundar mais no assunto lendo outras fontes e mesmo se não quiser se\naprofundar terá uma noção de como avaliar os modelos.\nContras\nEmbora o livro tenha um conteudo aparentemente bem abrangente, ele\nnão aprofunda nos temas o suficiente (principalmente na parte de\nmodelagem) como para termos um conhecimento básico em séries\ntemporais.\nAo terminar o livro, você não terá uma bagagem suficiente para poder\nescolher corretamente qual modelo utilzar (mas sabera como rodar os\nmodelos em R e/ou Python).\nO livro investe muito tempo na preparação dos dados (sendo algumas\ncoisas obvias até) e pouco tempo na parte de modelagem (apenas 4\ncapítulos e os temas são abordados de forma bastante introdutória).\nAlguns capítulos dão detalhe demais acerca dos códigos, detalhes que\napenas lendo o próprio código são fáceis de entender.\nO livro mistura conceitos básicos com conceitos mais avançados\n(cadeia de Markov, volatilidade, etc) mas não tenho certeza se alguém\niniciante sabera entender os conceitos mais avançados (mesmo que a\nautora explique brevemente os novos conceitos).\nNão todos os exemplos estão em ambas as linguagens, algumas vezes é\nutilizado apenas R e outras apenas\nPython.\nConclusão\nEu fui atrás do livro pensando incluir ele na bibliografia da minha\ndisciplina, mas após ter lido ele todo não o incluirei. O livro não\naborda os temas com a profundidade necessária (pelo menos não para\nalunos de estatística) mas é uma fonte interessante para ver código.\nAcho que o livro pode ser util para um leitor com conhecimento básico\nque queria lembrar alguns conceitos e ter uma noção dos avanços em\nMachine Learning, Deep Learning e pre-processamento de\ndados. Por outro lado, um usuário intermediário ou avançado achara o\nlivro repetitivo.\nCaso queira saber mais acerca do curso que ministrarei, pode ver a\ninformação aqui.\n\n\n\nNielsen, Aileen. 2019. Practical Time Series Analysis: Prediction\nwith Statistics and Machine Learning. O’Reilly Media.\n\n\n\n\n",
    "preview": "posts/2022-12-28-book-review-practical-time-series-analysis-prediction-with-statistics-and-machine-learning/ptsa.jpg",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-14-book-review-text-mining-with-r/",
    "title": "Book Review: Text Mining with R",
    "description": "Minhas impressões do livro \"Text Mining with R: a tidy approach\" da Julia Silge and David Robinson.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2022-03-14",
    "categories": [
      "Book Review",
      "R",
      "rstats",
      "Text Mining",
      "NLP"
    ],
    "contents": "\n\nContents\nIntrodução\nOverview do livro\nSobre os capítulos do livro\n\nO que mais/menos gostei\nComentários Finais\n\n\nIntrodução\nMineração de texto (ou text mining) e PLN (processamento da linguagem natural) não são minhas linhas de pesquisa, mas a raíz de um dos projetos do grupo de pesquisa BDAQ fiquei ainda mais curioso sobre como analisar texto.\nFoi assim que resolvi ler alguns livros sobre o assunto, sendo o primeiro deles o livro Text Mining with R: a tidy approach de Silge and Robinson (2017), que pode ser livremence acessado aqui e do qual trago algumas pequenas impressões para os alunos interessados no assunto.\nOverview do livro\n\n\n\n\n\nO livro possui uma versão impressa e pode ser adquirido aqui, mas os autores gentilmente disponibilizaram a versão online para ser acessada de forma gratuita aqui (sim, eu fui na gratuita!).\nO livro é uma introdução rápida e prática à mineração de texto, traz vários exemplos que podem ser facilmente reproduzidos através do código disponibilizado no próprio livro e apresenta diversos exemplos nos quais mineração de texto pode ser utilizada. Isto todo, utilizando o software R e alguns pacotes tais como tidytext, wordcloud, ggraph, entre muitos outros.\nSobre os capítulos do livro\nO livro possui 9 capítulos mais um de referências bibliográficas e a leitura é bastante fluida e rápida.\nOs quatro primeiros capítulos correspondem a conceitos básicos e são fundamentais para todo aquele que queira analisar texto.\nO capitulo 6 apresenta um tema mais avançado mas sem entrar no rigor matemático, apresentando apenas como e em que situações o método pode ser utilizado. Acredito que os leitores pouco familiarizados com machine learning ou análise multivariada terão maiores dificuldades para entender (pelo menos intuitivamente) como o método funciona, mas nada que atrapalhe a leitura do capítulo.\nComo não tudo no R pertence ao mundo tidyverse, o capítulo 5 apresenta como transitar entre os diferentes formatos utilizados para armazenar dados de texto de forma fácil e simples. Isto permite que seja possível utilizar diferentes pacotes destinados à análise de texto sem maiores preocupações.\nOs capítulos finais (7, 8 e 9) apresentam casos de estudo, onde tudo o visto no livro pode ser aplicado passo a passo em datasets reais.\nO que mais/menos gostei\nO ponto forte do livro é que ele pode ser lido, compreendido e aplicado ao mesmo tempo através dos códigos disponibilizados logo após a explicação de cada conceito. Isso faz com que ler o livro seja muito mais didático e agradável.\nO ponto negativo (embora não seja o foco do livro) é que os lexicos utilizados estão em inglês, o que faz com que sejam necessárias algumas adaptações para poder trabalhar com texto em português. Eu tentei contextualizar um pouco isso nesses slides que preparei para os alunos envolvidos no projeto de pesquisa que o Prof. Marcelo Castañeda (FACC/UFRJ) coordena.\nComentários Finais\nEm geral, o livro é muito bom (prático, direto, didático) e bastante útil para começar com análise de texto.\nO objetivo do livro é aprensentar os conceitos básicos, então não pense que após a leitura o livro será um Guru em mineração de texto.\nAlguns outros livros para se aprofundar no assunto são: Supervised Machine Learning for Text Analysis in R (Hvitfeldt and Silge 2021) que está disponibilizado livremente aqui e Textual Data Science with R (Bécue-Bertaut 2019)\n\nSe você gostar do R assim como eu, os códigos no livro serão muito úteis. Contudo, se você preferir alguma outra linguagem como Python ou Julia, o livro continua sendo útil e você pode focar em entender os conceitos e os exemplos de aplicação, bastando apenas como implementar os métodos na linguagem de preferência (devem existir diversas fontes onde pode encontrar isso.)\n\nHappy Coding!\n\n\n\n\nBécue-Bertaut, Monica. 2019. Textual Data Science with r. CRC Press.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in r. Chapman; Hall/CRC.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy Approach. O’Reilly Media, Inc.\n\n\n\n\n",
    "preview": "posts/2022-03-14-book-review-text-mining-with-r/tmwR.png",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {},
    "preview_width": 2100,
    "preview_height": 2756
  },
  {
    "path": "posts/2021-12-01-r-como-importar-dados-do-excel/",
    "title": "R: como importar dados do Excel?",
    "description": "No R é muito comum importar dados em formato .txt ou .csv. Contudo, muitas vezes estamos interessados em importar arquivos salvos em excel (.xls ou .xlsx). Neste post, apresento duas formas de ler dados vindos do Excel.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-12-01",
    "categories": [
      "R",
      "rstats"
    ],
    "contents": "\n\nContents\nIntrodução\nO pacote readxl\nO pacote xlsx\n\nIntrodução\nA forma mais fácil de importar dados do Excel é salvando os arquivos como .csv e depois importando esse dataset como usual.\n\n\n\n\n\n\n\n\n\nContudo, fazer isso não sempre é confortável (e se tiver muitos arquivos pode ser um pouco tedioso). Por isso, existem várias formas de importar os dados em excel, aqui veremos apenas duas formas.\n\nO pacote readxl\nO pacote readxl é uma ótima forma de importar dados em formato .xls ou .xlsx. Se quisermos importar um arquivo .xls utilizamos a função read_xls(), já se o arquivo tem extensão .xlsx utilizamos a função read_xlsx().\n\n\nlibrary(readxl)\narquivo_xls <- read_xls(\"nome_do_arquivo.xls\")\narquivo_xlsx <- read_xlsx(\"nome_do_outro_arquivo.xlsx\")\n\n\n\nSe tivermos um arquivo com várias abas, basta utilizar o argumento sheet (disponível em ambas as funções).\nNo seguinte exemplo, utilizaremos o arquivo NotasMAD211.xls, que contem duas abas (uma chamada P1 e outra chamada P2).\n\n\n\n\n\n\n\nlibrary(readxl)\nnotas_p1 <- read_xlsx(\"NotasMAD211.xlsx\", sheet = 1)\nhead(notas_p1)\n\n\n# A tibble: 6 × 2\n  ALUNO      Nota\n  <chr>     <dbl>\n1 Carlos      7.1\n2 Jose        7  \n3 Maria       6.6\n4 João       10  \n5 Beatriz     9  \n6 Guilherme   7  \n\n\n\n\n\n\n\n\nnotas_p2 <- read_xlsx(\"NotasMAD211.xlsx\", sheet = \"P2\")\nhead(notas_p2)\n\n\n# A tibble: 6 × 2\n  ALUNO      Nota\n  <chr>     <dbl>\n1 Carlos      7.1\n2 Jose        9  \n3 Maria      10  \n4 João        4  \n5 Beatriz     8.8\n6 Guilherme   9  \n\nO argumento sheet permite especificar ou posição da aba (na ordem em que ela aparece no arquivo) ou o nome da aba que queremos importar (o nome precisa estar entre aspas).\nO pacote xlsx\nAlgumas vezes, os arquivos com os quais trabalhamos estão protegidos por senha. Nesses casos, a função read.xlsx do pacote xlsx nos ajudará a importar os dados. read.xlsx funciona de forma parecida com as funções read_xls e read_xlsx.\nVamos supor que o arquivo NotasMAD211.xlsx esteja protegido por uma senha e que a senha seja BatatinhaFrita_123.\n\n\nlibrary(xlsx)\nnotas_p2 <- read.xlsx(\"NotasMAD211.xlsx\", sheetIndex = 2, password = \"BatatinhaFrita_123\")\n\n\n\nPronto! Os dados estão importados 🆒.\nObservações\nO argumento sheetIndex da função read.xlsx permite especificar a ordem da aba que queremos importar. Se quiser especificar o nome da aba, basta usar o argumento sheetName em lugar de sheetIndex.\nCuidado com arquivos de excel com filtros ativos. Já tive problemas carregando arquivos de excel quando tinha filtros ativos, a solução foi remover os filtros antes de importar o arquivo para o excel.\nHappy Coding!\n\n\n\n",
    "preview": "posts/2021-12-01-r-como-importar-dados-do-excel/ascsv001.png",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {},
    "preview_width": 289,
    "preview_height": 483
  },
  {
    "path": "posts/2021-07-27-criando-relatorios-no-rstudio/",
    "title": "Criando relatorios no Rstudio",
    "description": "Escrever relatórios científicos pode ser um trabalho tedioso, mas existem formas que deixam o trabalho um pouco mais prazeroso. Uma delas é o Rmarkdown, que nos ajuda a escrever relatórios de forma fácil e com uma apresentação mais profissional (além de permitir incluir código, figuras e tabelas diretamente do R).  Neste post explicarei os primeiros passos para fazer relatórios com o Rstudio.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-07-27",
    "categories": [
      "Rstudio",
      "Rmarkdown",
      "Latex",
      "Relatorios",
      "rstats"
    ],
    "contents": "\n\nContents\nIntrodução\nConfiguração\nPrimeiros passos\n\n\n\n\nIntrodução\nEscrever relatórios e artigos científicos são tarefas que fazem parte da vida de todo pesquisador. Alguns preferem utilizar word, que é um editor de texto amplamente conhecido. Já outros, como eu, preferem utilizar que nos permite escrever fórmulas matemáticas de forma fácil e elegante.\nContudo, essas duas opções não são as únicas no mercado. Se você gostar do R/Rstudio, quer utilizar um formato elegante, quer a flexibilidade de trocar entre pdf e word facilmente e ainda quer facilitar sua vida editando tabelas e gráficos… você precisa utilizar o Rmarkdown! 🕺.\nConfiguração\nPara poder fazer os relatórios, precisa de configurar o Rstudio primeiro. Se nunca fez isto antes, leia este post aqui\nPrimeiros passos\nUma vez configurado o Rstudio, assista este video onde explico passo a passo como fazer os relatórios (basta fazer clic na imagem).\n\nComo pode ver, a forma de escrever é muito fácil e simples, basta escrever da mesma forma que escrevemos em qulquer editor de texto e incluir alguns pequenos detalhes para aquelas coisas que são um pouco mais complexas.\nRecomendo que imprima e tenha sempre por perto o seguinte Cheat Sheet, que é um resumo das coisas básicas que precisa saber para fazer seus relatórios. Se precisar de coisas mais avançadas, tem esse outro Cheat Sheet que pode lhe ajudar. Para uma guia completa do Rmarkdown, entre aqui.\n\nEm um próximo post explicarei algumas outras coisas (incluir código, figuras, tabelas, etc).\n\nHappy Coding!\n\n\n\n",
    "preview": "posts/2021-07-27-criando-relatorios-no-rstudio/rmarkdown.png",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {},
    "preview_width": 400,
    "preview_height": 257
  },
  {
    "path": "posts/2021-07-24-utilizando-o-r-para-ler-tabelas-em-pdf/",
    "title": "Utilizando o R para ler tabelas em PDF",
    "description": "É possível trazer para o R dados/tabelas vindos de um arquivo em PDF?  Neste post eu compartilho minha experiência com o pacote `tabulizer`, que nos ajudará a extrair tabelas em PDF e importá-las como _datasets_ no R.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-07-24",
    "categories": [
      "Rstudio",
      "Git",
      "GitHub",
      "rstats"
    ],
    "contents": "\n\nContents\nIntrodução\nConfiguração\nExtraindo a tabela do PDF\nComentários finais\n\n\n\n\nIntrodução\nRecentemente, alguns alun@s me perguntaram se era possível importar no R dados vindos de um arquivo PDF. Honestamente, nunca tive a necessidade de fazer isso, mas sabia que certamente existia algum pacote para isso no R.\nPesquisando pela internet encontrei algumas opções, entre elas, o pacote tabulizer (Leeper 2018).\nConfiguração\nO processo de extração será feito com o pacote tabulizer, que precisa do pacote rJava, que por sua vez, precisa ter o JDK (Java Developer Kit) instalado.\nPara instalar o necessário, siga os seguintes passos:\nInstalar o Java Developer Kit (Link para baixar aqui)\nPara usuários do Mac, digitar no terminal sudo R CMD javareconf (se você utilizar windos, talvez este post ajude)\nInstalar no R os pacotes rJava, tabulizerjars e tabulizer,\n\n\ninstall.packages(\"rJava\")\ninstall.packages(\"tabulizerjars\")\ninstall.packages(\"tabulizer\")\n\n\n\nExtraindo a tabela do PDF\nApós a configuração, carregamos os pacotes rJava, tabulizerjars e tabulizer no R. Se a configuração anterior foi bem sucedida, os pacotes carregarão sem nenhum problema.\n\n\nlibrary(rJava)\nlibrary(tabulizerjars)\nlibrary(tabulizer)\n\n\n\nPara extrair as tabelas, utilizaremos a função extract_tables(). Por padrão, esta função tem o argumento guess = TRUE o que significa que a função vai adivinhar a localização da tabela no PDF (mas se quisermos ser mais específicos, teremos que mudar alguns dos parâmetros. Para mais detalhes ver help(extract_tables)).\nA maneira de exemplo, utilizaremos o seguinte PDF e para importar os dados contidos nas tabelas, utilizaremos os seguintes comandos.\n\n\nendereco_arquivo <- \"https://www.math.arizona.edu/~jwatkins/normal-table.pdf\"\ntabelas <- extract_tables(file   = endereco_arquivo, output = \"data.frame\")\n\n\n\nNote que o objeto tabelas é uma lista com dois elementos, um por cada tabela.\n\n\nclass(tabelas)\n\n\n[1] \"list\"\n\nlength(tabelas)\n\n\n[1] 2\n\nSe quisermos acessar a alguma das tabelas, basta fazer\n\n\nlibrary(dplyr)\nlibrary(purrr)\ntabelas %>% pluck(1) %>% as_tibble()  # Primeira tabela\n\n\n# A tibble: 35 x 11\n       z  X0.00  X0.01  X0.02  X0.03  X0.04  X0.05  X0.06  X0.07\n   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1  -3.4 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003\n 2  -3.3 0.0005 0.0005 0.0005 0.0004 0.0004 0.0004 0.0004 0.0004\n 3  -3.2 0.0007 0.0007 0.0006 0.0006 0.0006 0.0006 0.0006 0.0005\n 4  -3.1 0.001  0.0009 0.0009 0.0009 0.0008 0.0008 0.0008 0.0008\n 5  -3   0.0013 0.0013 0.0013 0.0012 0.0012 0.0011 0.0011 0.0011\n 6  -2.9 0.0019 0.0018 0.0018 0.0017 0.0016 0.0016 0.0015 0.0015\n 7  -2.8 0.0026 0.0025 0.0024 0.0023 0.0023 0.0022 0.0021 0.0021\n 8  -2.7 0.0035 0.0034 0.0033 0.0032 0.0031 0.003  0.0029 0.0028\n 9  -2.6 0.0047 0.0045 0.0044 0.0043 0.0041 0.004  0.0039 0.0038\n10  -2.5 0.0062 0.006  0.0059 0.0057 0.0055 0.0054 0.0052 0.0051\n# … with 25 more rows, and 2 more variables: X0.08 <dbl>, X0.09 <dbl>\n\ntabelas %>% pluck(2) %>% as_tibble()  # Segunda tabela\n\n\n# A tibble: 35 x 11\n       z X0.00 X0.01 X0.02 X0.03 X0.04 X0.05 X0.06 X0.07 X0.08 X0.09\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1   0   0.5   0.504 0.508 0.512 0.516 0.520 0.524 0.528 0.532 0.536\n 2   0.1 0.540 0.544 0.548 0.552 0.556 0.560 0.564 0.568 0.571 0.575\n 3   0.2 0.579 0.583 0.587 0.591 0.595 0.599 0.603 0.606 0.610 0.614\n 4   0.3 0.618 0.622 0.626 0.629 0.633 0.637 0.641 0.644 0.648 0.652\n 5   0.4 0.655 0.659 0.663 0.666 0.67  0.674 0.677 0.681 0.684 0.688\n 6   0.5 0.692 0.695 0.698 0.702 0.705 0.709 0.712 0.716 0.719 0.722\n 7   0.6 0.726 0.729 0.732 0.736 0.739 0.742 0.745 0.749 0.752 0.755\n 8   0.7 0.758 0.761 0.764 0.767 0.770 0.773 0.776 0.779 0.782 0.785\n 9   0.8 0.788 0.791 0.794 0.797 0.800 0.802 0.805 0.808 0.811 0.813\n10   0.9 0.816 0.819 0.821 0.824 0.826 0.829 0.832 0.834 0.836 0.839\n# … with 25 more rows\n\nou equivalentemente (sem utilizar o tidyway)\n\n\ntabelas[[1]]   # Primeira tabela\n\n\n      z  X0.00  X0.01  X0.02  X0.03  X0.04  X0.05  X0.06  X0.07\n1  -3.4 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003\n2  -3.3 0.0005 0.0005 0.0005 0.0004 0.0004 0.0004 0.0004 0.0004\n3  -3.2 0.0007 0.0007 0.0006 0.0006 0.0006 0.0006 0.0006 0.0005\n4  -3.1 0.0010 0.0009 0.0009 0.0009 0.0008 0.0008 0.0008 0.0008\n5  -3.0 0.0013 0.0013 0.0013 0.0012 0.0012 0.0011 0.0011 0.0011\n6  -2.9 0.0019 0.0018 0.0018 0.0017 0.0016 0.0016 0.0015 0.0015\n7  -2.8 0.0026 0.0025 0.0024 0.0023 0.0023 0.0022 0.0021 0.0021\n8  -2.7 0.0035 0.0034 0.0033 0.0032 0.0031 0.0030 0.0029 0.0028\n9  -2.6 0.0047 0.0045 0.0044 0.0043 0.0041 0.0040 0.0039 0.0038\n10 -2.5 0.0062 0.0060 0.0059 0.0057 0.0055 0.0054 0.0052 0.0051\n11 -2.4 0.0082 0.0080 0.0078 0.0075 0.0073 0.0071 0.0069 0.0068\n12 -2.3 0.0107 0.0104 0.0102 0.0099 0.0096 0.0094 0.0091 0.0089\n13 -2.2 0.0139 0.0136 0.0132 0.0129 0.0125 0.0122 0.0119 0.0116\n14 -2.1 0.0179 0.0174 0.0170 0.0166 0.0162 0.0158 0.0154 0.0150\n15 -2.0 0.0228 0.0222 0.0217 0.0212 0.0207 0.0202 0.0197 0.0192\n16 -1.9 0.0287 0.0281 0.0274 0.0268 0.0262 0.0256 0.0250 0.0244\n17 -1.8 0.0359 0.0351 0.0344 0.0336 0.0329 0.0322 0.0314 0.0307\n18 -1.7 0.0446 0.0436 0.0427 0.0418 0.0409 0.0401 0.0392 0.0384\n19 -1.6 0.0548 0.0537 0.0526 0.0516 0.0505 0.0495 0.0485 0.0475\n20 -1.5 0.0668 0.0655 0.0643 0.0630 0.0618 0.0606 0.0594 0.0582\n21 -1.4 0.0808 0.0793 0.0778 0.0764 0.0749 0.0735 0.0721 0.0708\n22 -1.3 0.0968 0.0951 0.0934 0.0918 0.0901 0.0885 0.0869 0.0853\n23 -1.2 0.1151 0.1131 0.1112 0.1093 0.1075 0.1056 0.1038 0.1020\n24 -1.1 0.1357 0.1335 0.1314 0.1292 0.1271 0.1251 0.1230 0.1210\n25 -1.0 0.1587 0.1562 0.1539 0.1515 0.1492 0.1469 0.1446 0.1423\n26 -0.9 0.1841 0.1814 0.1788 0.1762 0.1736 0.1711 0.1685 0.1660\n27 -0.8 0.2119 0.2090 0.2061 0.2033 0.2005 0.1977 0.1949 0.1922\n28 -0.7 0.2420 0.2389 0.2358 0.2327 0.2296 0.2266 0.2236 0.2206\n29 -0.6 0.2743 0.2709 0.2676 0.2643 0.2611 0.2578 0.2546 0.2514\n30 -0.5 0.3085 0.3050 0.3015 0.2981 0.2946 0.2912 0.2877 0.2843\n31 -0.4 0.3446 0.3409 0.3372 0.3336 0.3300 0.3264 0.3228 0.3192\n32 -0.3 0.3821 0.3783 0.3745 0.3707 0.3669 0.3632 0.3594 0.3557\n33 -0.2 0.4207 0.4168 0.4129 0.4090 0.4052 0.4013 0.3974 0.3936\n34 -0.1 0.4602 0.4562 0.4522 0.4483 0.4443 0.4404 0.4364 0.4325\n35  0.0 0.5000 0.4960 0.4920 0.4880 0.4840 0.4801 0.4761 0.4721\n    X0.08  X0.09\n1  0.0003 0.0002\n2  0.0004 0.0003\n3  0.0005 0.0005\n4  0.0007 0.0007\n5  0.0010 0.0010\n6  0.0014 0.0014\n7  0.0020 0.0019\n8  0.0027 0.0026\n9  0.0037 0.0036\n10 0.0049 0.0048\n11 0.0066 0.0064\n12 0.0087 0.0084\n13 0.0113 0.0110\n14 0.0146 0.0143\n15 0.0188 0.0183\n16 0.0239 0.0233\n17 0.0301 0.0294\n18 0.0375 0.0367\n19 0.0465 0.0455\n20 0.0571 0.0559\n21 0.0694 0.0681\n22 0.0838 0.0823\n23 0.1003 0.0985\n24 0.1190 0.1170\n25 0.1401 0.1379\n26 0.1635 0.1611\n27 0.1894 0.1867\n28 0.2177 0.2148\n29 0.2483 0.2451\n30 0.2810 0.2776\n31 0.3156 0.3121\n32 0.3520 0.3483\n33 0.3897 0.3859\n34 0.4286 0.4247\n35 0.4681 0.4641\n\ntabelas[[1]]   # Segunda tabela\n\n\n      z  X0.00  X0.01  X0.02  X0.03  X0.04  X0.05  X0.06  X0.07\n1  -3.4 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003\n2  -3.3 0.0005 0.0005 0.0005 0.0004 0.0004 0.0004 0.0004 0.0004\n3  -3.2 0.0007 0.0007 0.0006 0.0006 0.0006 0.0006 0.0006 0.0005\n4  -3.1 0.0010 0.0009 0.0009 0.0009 0.0008 0.0008 0.0008 0.0008\n5  -3.0 0.0013 0.0013 0.0013 0.0012 0.0012 0.0011 0.0011 0.0011\n6  -2.9 0.0019 0.0018 0.0018 0.0017 0.0016 0.0016 0.0015 0.0015\n7  -2.8 0.0026 0.0025 0.0024 0.0023 0.0023 0.0022 0.0021 0.0021\n8  -2.7 0.0035 0.0034 0.0033 0.0032 0.0031 0.0030 0.0029 0.0028\n9  -2.6 0.0047 0.0045 0.0044 0.0043 0.0041 0.0040 0.0039 0.0038\n10 -2.5 0.0062 0.0060 0.0059 0.0057 0.0055 0.0054 0.0052 0.0051\n11 -2.4 0.0082 0.0080 0.0078 0.0075 0.0073 0.0071 0.0069 0.0068\n12 -2.3 0.0107 0.0104 0.0102 0.0099 0.0096 0.0094 0.0091 0.0089\n13 -2.2 0.0139 0.0136 0.0132 0.0129 0.0125 0.0122 0.0119 0.0116\n14 -2.1 0.0179 0.0174 0.0170 0.0166 0.0162 0.0158 0.0154 0.0150\n15 -2.0 0.0228 0.0222 0.0217 0.0212 0.0207 0.0202 0.0197 0.0192\n16 -1.9 0.0287 0.0281 0.0274 0.0268 0.0262 0.0256 0.0250 0.0244\n17 -1.8 0.0359 0.0351 0.0344 0.0336 0.0329 0.0322 0.0314 0.0307\n18 -1.7 0.0446 0.0436 0.0427 0.0418 0.0409 0.0401 0.0392 0.0384\n19 -1.6 0.0548 0.0537 0.0526 0.0516 0.0505 0.0495 0.0485 0.0475\n20 -1.5 0.0668 0.0655 0.0643 0.0630 0.0618 0.0606 0.0594 0.0582\n21 -1.4 0.0808 0.0793 0.0778 0.0764 0.0749 0.0735 0.0721 0.0708\n22 -1.3 0.0968 0.0951 0.0934 0.0918 0.0901 0.0885 0.0869 0.0853\n23 -1.2 0.1151 0.1131 0.1112 0.1093 0.1075 0.1056 0.1038 0.1020\n24 -1.1 0.1357 0.1335 0.1314 0.1292 0.1271 0.1251 0.1230 0.1210\n25 -1.0 0.1587 0.1562 0.1539 0.1515 0.1492 0.1469 0.1446 0.1423\n26 -0.9 0.1841 0.1814 0.1788 0.1762 0.1736 0.1711 0.1685 0.1660\n27 -0.8 0.2119 0.2090 0.2061 0.2033 0.2005 0.1977 0.1949 0.1922\n28 -0.7 0.2420 0.2389 0.2358 0.2327 0.2296 0.2266 0.2236 0.2206\n29 -0.6 0.2743 0.2709 0.2676 0.2643 0.2611 0.2578 0.2546 0.2514\n30 -0.5 0.3085 0.3050 0.3015 0.2981 0.2946 0.2912 0.2877 0.2843\n31 -0.4 0.3446 0.3409 0.3372 0.3336 0.3300 0.3264 0.3228 0.3192\n32 -0.3 0.3821 0.3783 0.3745 0.3707 0.3669 0.3632 0.3594 0.3557\n33 -0.2 0.4207 0.4168 0.4129 0.4090 0.4052 0.4013 0.3974 0.3936\n34 -0.1 0.4602 0.4562 0.4522 0.4483 0.4443 0.4404 0.4364 0.4325\n35  0.0 0.5000 0.4960 0.4920 0.4880 0.4840 0.4801 0.4761 0.4721\n    X0.08  X0.09\n1  0.0003 0.0002\n2  0.0004 0.0003\n3  0.0005 0.0005\n4  0.0007 0.0007\n5  0.0010 0.0010\n6  0.0014 0.0014\n7  0.0020 0.0019\n8  0.0027 0.0026\n9  0.0037 0.0036\n10 0.0049 0.0048\n11 0.0066 0.0064\n12 0.0087 0.0084\n13 0.0113 0.0110\n14 0.0146 0.0143\n15 0.0188 0.0183\n16 0.0239 0.0233\n17 0.0301 0.0294\n18 0.0375 0.0367\n19 0.0465 0.0455\n20 0.0571 0.0559\n21 0.0694 0.0681\n22 0.0838 0.0823\n23 0.1003 0.0985\n24 0.1190 0.1170\n25 0.1401 0.1379\n26 0.1635 0.1611\n27 0.1894 0.1867\n28 0.2177 0.2148\n29 0.2483 0.2451\n30 0.2810 0.2776\n31 0.3156 0.3121\n32 0.3520 0.3483\n33 0.3897 0.3859\n34 0.4286 0.4247\n35 0.4681 0.4641\n\ne pronto!, nossos dados do PDF estão agora no R!.\nComentários finais\nSe você tiver uma versão do JDK mais recente, provavelmemte o Rstudio vai travar quando tentar utilizar a função extract_tables().\nPara corrigir o problema, basta desinstalar a versão mais recente e instalar a versão 11\nSe não quiser desinstalar a versão mais atual do JDK, instale a versão 11 sem desinstalar a outra versão e no terminal do R escreva\n\n\nSys.setenv(JAVA_HOME='/Library/Java/JavaVirtualMachines/jdk-11.0.12.jdk/Contents/Home')\n\n\n\nem que /Library/Java/JavaVirtualMachines/jdk-11.0.12.jdk é o endereço onde a versão 11.0.12 foi instalada.\n\nNota: Ainda não testei a função com outras tabelas, então não sei que tão bem funciona com tabelas mais complexas.\n\nHappy Coding!\n\n\n\nLeeper, Thomas J. 2018. Tabulizer: Bindings for Tabula PDF Table Extractor Library.\n\n\n\n\n",
    "preview": "posts/2021-07-24-utilizando-o-r-para-ler-tabelas-em-pdf/pexels-artem-podrez-6779714.jpg",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-18-criar-um-novo-projetorepo-no-rstudiogithub/",
    "title": "Criar um novo projeto/repo no Rstudio/GitHub",
    "description": "Passo a passo para criar um novo projeto no Rstudio e vinculá-lo ao GitHub.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-06-18",
    "categories": [
      "Rstudio",
      "Git",
      "GitHub",
      "rstats"
    ],
    "contents": "\n\nContents\nIntrodução\nPré-requisitos.\nCriando um novo projeto.\n\n\n\n\nIntrodução\nNo meu post anterior, falei um pouco de como configurar o Rstudio para ele estar vinculado ao nosso Github. Hoje explicarei como criar um novo projeto no Rstudio e vinculá-lo ao GitHub (sem precisar clonar um repositório existente como vimos no post anterior).\nPré-requisitos.\nTer sua conta do Git/GitHub configurada com o Rstudio.\nCriando um novo projeto.\nPara criar um novo projeto:\n\n\nusethis::create_project(\"/Volumes/CTRUCIOS_SD/Research/repo_teste\")\n\n\n\nrepare que /Volumes/CTRUCIOS_SD/Research/ é o local onde quero criar o novo projeto e repo_teste é o nome do novo projeto.\nAssim que rodar o código acima, aparecerá algo parecido com:\n\n\n\nLogo após, o Rstudio abrirá outra sessão com o novo projeto. Se não abriu, vá na pasta onde criou o projeto, no meu caso /Volumes/CTRUCIOS_SD/Research/, e encontrará uma pasta com o nome repo_teste onde encontrará o projeto do Rstudio (arquivo repo_teste.Rproj).\nNessa nova sessão do Rstudio, repare que o nome do novo projeto aparece no canto superior direito. (Aqui já pode fechar todas as outras sessões abertas do Rstudio e ficar apenas com a do nosso novo projeto).\n\n\n\nAgora precisamos vincular esse novo projeto ao Git/GitHub. Primeiro vincularemos o projetoco Git.\n\n\nusethis::use_git() \n\n\n\n\n\n\n\n\n\nUma vez vinculado o projeto ao Git, vinculamos ele ao GitHub.\n\n\nusethis::use_github()\n\n\n\n\n\n\nSe tudo de certo, vá no teu Github e terá o repositório lá.\n\n\n\nPronto! Seu novo projeto foi criado, e vinculado com o Git/GitHub.\nHappy Coding!\n\n\n\n",
    "preview": "posts/2021-06-18-criar-um-novo-projetorepo-no-rstudiogithub/im/original.png",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {},
    "preview_width": 896,
    "preview_height": 896
  },
  {
    "path": "posts/2021-06-12-git-github-e-rstudio/",
    "title": "Git, GitHub e Rstudio",
    "description": "Tutorial de como configurar o Git/GitHub no Rstudio.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-06-12",
    "categories": [
      "Rstudio",
      "rstats",
      "Git",
      "GitHub"
    ],
    "contents": "\n\nContents\nIntrodução\nPré-requisitos\nPrimeiros Passos\nIniciando a configuração\nVinculando um repositório ao Rstudio\nBonus: commit and push\nEDIT:\n\nIntrodução\nSe você escreve código (seja na linguagem que for) sabe que utilizar o Git/GitHub é muito útil. Se você (assim como eu) gosta de usar o Rstudio, talvez esteja interessado(a) em configurar sua conta do Git no Rstudio.\nEsta configuração é algo que é feito apenas uma única vez na vida (ok não na vida, mas uma única vez por computador) e este tutorial vai te guiar no passo a passo.\nPré-requisitos\nAntes de começar, existem algumas coisas que devemos fazer antes (caso ainda não tenha feito):\nInstalar o R (Baixar aqui)\nInstalar o Rstudio (Baixar aqui)\nCriar uma conta no Github (criar conta aqui)\nInstalar o Git (Baixar aqui)\nPrimeiros Passos\nNo Rstudio instalar e carregar o pacote usethis\n\n\ninstall.packages(\"usethis\")\nlibrary(usethis)\n\n\n\nPara configurar sua conta do Git no Rstudio precisará de um nome de usuário e do email utilizado na conta do Github.\n\n\nusethis::use_git_config(user.name = \"nome_usuario_aqui\", \n                        user.email = \"email_github_aqui@gmail.com\") \n\n\n\nIniciando a configuração\nNo Rstudio escreva\n\n\nusethis::create_github_token()\n\n\n\nisto abrirá no seu navegador algo mais ou menos assim:\n\n\n\nVocê pode escrever o nome que você quiser (por exemplo RstudioNotebook, RstudioCasa, etc) mas recomendo que seja um nome que lhe ajude a lembrar para que esse token é utilizado. Eu deixei todos os valores por padrão e no final da página basta clicar em Generate token.\n\n\n\nLogo após, aparecera o token, o qual você precisa copiar (click no icone azul bem do lado do token). Esse token será necessário para terminar a configuração do Git com o Rstudio.\n\n\n\nAgora escreva\n\n\nusethis::edit_r_environ()\n\n\n\nO Rstudio abrirá um arquivo chamado .Renviron, nele escreva o token que acabou de ser gerado e depois dê um enter (para pular linha).\n\n\n\nSalve o arquivo e reinicie sessão (se preferir pode fechar o Rstudio e abri-lo de novo)\n\n\n.rs.restartR()\n\n\n\n\nPronto! Agora já está configurado.\n\nVinculando um repositório ao Rstudio\nA forma mais fácil de trabalhar com um projeto no Rstudio e que este esteja vinculado ao GitHub é clonando um reposítorio existente. Para isto, precisamos primeiro criar um repositório no GitHub e depois cloná-lo.\nCriando o repositório no GitHUb\nNo GitHub, vá na aba Repositories e dê clic em New\n\n\n\nAparecerá a janela abaixo e você pode escolher se criar um repositório público ou privado e se adicionar um README ou nao. Aqui, eu escolhi a opção privado e adicionar README. Após escolher as opções da sua preferêcia clic em Create repository.\n\n\n\nCom o repositório criado, dê um clic na setinha do botão Code e copie o endereço que aparecer.\n\n\n\nClonando o repositorio.\nAgora estamos chegando na etapa final. Abra o Rstudio e crie um novo projeto File > New Project... e escolha a opção Version Control.\n\n\n\nEnseguida escolha a opção Git e preencha a opção Repository URL com o endereço que acabou de copiar do GitHub. Escolha também onde quer criar o projeto no seu computador (no meu caso /Volumes/CTRUCIOS_SD/Research). Não esqueça marcar a opção Open in new session.\n\n\n\n\n\n\nClicar em Create project e Pronto! Seu repositório no GitHub e seu novo projeto do Rstudio estão vinculados, agora é só escrever seus códigos e fazer commits, Push and Pulls.\nBonus: commit and push\nO código que você escreverá no seu computador, ficará unicamente no seu computador. Por isso, não esqueça fazer commit e Push de tempos em tempos para salvar no GitHub seu código mais recente.\nPara mostrar como se fazer commit and Push, criei um arquivo chamado codigo01.R\n\n\n\nPara fazer commit, vá na aba Git e dê clic na opção Commit.\n\n\n\nAgora marque os arquivos que quer subir para o GitHub (no nosso caso, o arquivo codigo01.R) e escreva uma breve descrição do que foi feito no código (para você lembrar).\n\n\n\nClic em Commit e depois clic em Push.\nSe tudo deu certo, seu código aparecerá no seu repositório do GitHub.\n\n\n\nNota: Se o arquivo codigo01.R não aparecer no GitHub, tente fazer um Push de novo, se lhe pedir usuario e senha, coloque seu nome de usuário do GitHub e quando pedir a senha coloque o token que foi gerado (no começo deste post).\n\n\n\nEDIT:\nCaso Rstudio peça seu usuário e senha constantemente, tente com a função gitcreds_set() do pacote gitcreds.\n\n\ngitcreds::gitcreds_set()\n\n\n\nApós rodar o comando, basta seguir os passos e inserir seu token.\nEspero ter ajudado. Happy Coding!\n\n\n\n",
    "preview": "posts/2021-06-12-git-github-e-rstudio/im/001.png",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {},
    "preview_width": 1340,
    "preview_height": 688
  },
  {
    "path": "posts/2021-05-01-configurando-rstudio-para-usar-o-rmarkdown/",
    "title": "Configurando Rstudio para usar o Rmarkdown",
    "description": "Passo a passo de como configurar o Rstudio para fazer relatórios e apresentações utlizando o Rmarkdown.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-05-01",
    "categories": [
      "Rstudio",
      "Rmarkdown",
      "rstats"
    ],
    "contents": "\n\nContents\nIntrodução\nPasso a Passo\nConclusões\n\nIntrodução\nFazer relatórios técnicos, apresentações e escrever artigos é parte do dia a dia tanto das pessoas envolvidas com pesquisa quanto das envolvidas com análise de dados.\nFazer estas tarefas pode ser cansativo e, inclussíve, levar a erros na digitação dos resultados.\nUma forma eficiente e fácil de fazermos isto é misturar a funcionalidade do R com a versatilidade de fazer relatórios utilizando o Rmarkdown.\nA seguir, descreverei passo a passo como configurar o Rstudio para poder fazer apresentações/relatórios utilizando o Rmarkdown1.\nPasso a Passo\nSe é a primeira vez que está tentando fazer um relatório/apresentação e está em dúvida sobre como proseguir, segue um passo e passo detalhado\nPasso 1\nQuando abrir o Rstudio, verá uma tela semelhante com essa aqui:\n\n\n\nPasso 2\nCrie um script markdown:\n\nFile > New File > R Markdown\n\n\n\n\nAssim que criar o script aparecerá a seguinte mensagem\n\n\n\nbasta clicar em Yes e deixar que todos os pacotes sejam instalados (você verá algo como a tela a seguir).\n\n\n\nPasso 3\nApós instalar todos os pacotes, tente mais uma vez criar o script\n\nFile > New File > R Markdown\n\nPor padrão, a opção Document estará ativa e se tudo estiver certo, você verá na tela o seguinte:\n\n\n\nse você escolher a opção Presentation, verá a seguinte:\n\n\n\nEm ambas as opções você pode escolher como fará o relatório/apresentação (html, pdf ou word/power point). Escolha a opção PDF ou PDF (Beamer)2\nPasso 4\nIndependente da opção escolhida (Document ou Presentation) aparecerá um template pronto para rodar. Basta você clicar no botão Knit\n\n\n\n(esse botão é para compilar tudo o escrito no seu relatório/apresentação e utilizará ele frequentemente)\nSe você for usuário de Latex, já deve ter o miktex instalado e o Rmarkdown deveria funcionar normalmente. Caso não seja usuário de Latex, continue com os próximos passos.\nPasso 5\nQuando apertar o botão pela primeira vez, caso você não tenha instalado antes o miktex, o arquivo não compilará e aparecerá algo parecido com:\n\n\n\nO que importa para nós é a seguinte mensagem:\n\n\n\nOu seja, precisamos instalar mais algumas coisinhas antes de podermos fazer apresentações/relatórios utilizando o Rstudio.\nBasta rodar o comando\n\n\ntinytex::install_tinytex()\n\n\n\nque tudo o necessário será instalado (você verá uma janela como a seguinte)\n\n\n\nPasso 6\nAssim que a instalação anterior tiver terminado, aparecerá a seguinte mensagem\n\n\n\nmas não se preocupe, apenas ignore.\nPasso 7\nPronto! já pode utilizar o Rstudio para fazer relatórios. Para testar, aperte o botão knit\n\n\n\ne o pdf de exemplo será compilado!\n\n\n\nConclusões\nRstudio é uma excelente IDE para se trabalhar com R, mas agora podemos além de escrever nosso código, fazer relatórios/apresentações\nPara poder utilizar basta apenas seguir alguns passos simples de configuração\nSe você já tiver instalado o Latex no seu computador, pode pular os passos 5 e 6, e ir direto para o passo 7\nCaso não saiba o que é Latex (isso significa que não tem o Latex nem miktex instalados) não se preocupe, basta seguir os passos 5 e 6 que tudo ficará pronto para utilizar o Rstudio para fazer relatórios/apresentações\n\nOs passos são bem intuitivos e a maioria de pessoas talvez não precise deste tutorial, mas, caso alguém precisar, deixo aqui o passo a passo↩︎\nPode escolher qualquer outra opção mas eu prefiro fazer em PDF. Ah, eu nunca testei a opção word/power point mas deveria funcionar↩︎\n",
    "preview": "posts/2021-05-01-configurando-rstudio-para-usar-o-rmarkdown/im/001.png",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {},
    "preview_width": 1366,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-01-minimos-quadrados-ordinarios/",
    "title": "Mínimos Quadrados Ordinários",
    "description": "Quando trabalhamos com modelos de regressão é necessário estimar os parâmetros do modelo. Um dos métodos de estimação mais conhecidos é o estimador de mínimos quadrados ordinários (ou MQO para os amigos). Em este post discutiremos a necessidade de estimar os parâmetros, entendermos a intuição por trás do método e derivaremos, passo a passo, o estimador MQO no modelo de regressão linear simples.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-04-01",
    "categories": [
      "Linear Regression",
      "Proof"
    ],
    "contents": "\n\nContents\nIntrodução\nA intuição por trás do estimador MQO\nDerivando o estimador MQO\nPrimeira derivada\nIgualando a zero\nVerificando que é ponto de mínimo\nEstimador MQO\n\nConclusões\n\nIntrodução\nO modelo de regressão linear simples é um dos modelos mais simples de Statistical Learning / Econometria. O modelo assume que a relação entre as variáveis \\(Y\\) e \\(X\\) é dada por: \\[\\begin{equation}\\label{RLS}\nY = \\beta_0 + \\beta_1 X + u\n\\end{equation}\\] em que \\(\\beta_0\\) e \\(\\beta_1\\) são os parâmetros do modelo e \\(u\\) é um termo aleatório.\nSe conhecermos \\(\\beta_0\\) e \\(\\beta_1\\), basta utilizar o modelo para um determinado valor de \\(X\\), digamos \\(x\\), e saberemos o valor esperado de \\(Y\\) dado \\(x\\)1.\nInfelizmente, nunca conhecemos \\(\\beta_0\\) e \\(\\beta_1\\) e então precisamos estimar esses valores utilizando os dados da nossa amostra.\nExistem vários métodos de estimação, mas hoje discutiremos o estimador de mínimos quadrados ordinários (MQO).\nA intuição por trás do estimador MQO\nSejam \\((y_1, x_1), \\ldots, (y_n, x_n)\\) os elementos de uma amostra aleaatória (a.a) de tamanho \\(n\\) extraida de \\((Y,X)\\). Estamos interessados em estimar os parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) na reta de regressão, mas…qual reta escolher?\nA Figura 1 apresenta o gráfico de dispersão de educação (\\(x\\)) vs. renda (\\(y\\)). Como podemos ver, existem várias retas (na verdade existem infinitas retas) que podemos tracejar na nossa tentativa de estimar \\(\\beta_0\\) e \\(\\beta_1\\).\n\n\n\nFigure 1: Gráfico de dispersão education vs. income\n\n\n\nEntre os vários critérios que poderíamos escolher para escolher a reta, vamos escolher a reta que minimiza a soma de quadrados dos resíduos. Mas… quem são esses resíduos? os resíduos, denotados por \\(\\hat{u}_i\\), são definidos como \\(\\hat{u}_i = y_i - \\hat{y_i}\\) com \\(\\hat{y}_i = b_0 + b_1x_i\\). Ou seja, independente dos valores \\(b_0\\) e \\(b_1\\) que escolhermos, os resíduos são a diferença entre o valor verdadeiro (\\(y_i\\)) e o estimado (\\(\\hat{y}_i\\)).\nOs valores \\(b_0\\) e \\(b_1\\) que minimizem a SQR vamos denotá-los como \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\). Em termos matemáticos, queremos \\(b_0\\) e \\(b_1\\) tal que \\[\\hat{\\beta}_0, \\hat{\\beta}_1 = \\mathop{\\mathrm{argmin}}\\limits_{b_0, b_1} SQR\\] em que \\[SQR := \\displaystyle \\sum_{i=1}^n \\hat{u}_i^2 \\equiv \\displaystyle \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\equiv \\displaystyle \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\\]\nDerivando o estimador MQO\nNosso problema de encontrar \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) resume-se, então, a um problema de minimização, onde a função a minimizar é \\[SQR := \\displaystyle \\sum_{i=1}^n (\\underbrace{y_i - b_0 - b_1 x_i}_{\\hat{u}_i})^2\\]\nPara resolver nosso problema de minimização vamos a lembrar um pouco das aulas de cálculo. Vamos a calcular a primeira derivada, igualar a zero para obter os valores candidatos e finalmente, vamos a calcular a segunda derivada para verificar que efetivamente os valores encontrados são valores que minimizam a função.\nPrimeira derivada\nDerivando SQR em relação a \\(b_0\\) temos\n\\[\\begin{equation}\n\\dfrac{\\partial SQR}{\\partial b_0}   = -2 \\displaystyle \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)  = -2 n \\big( \\bar{y} - b_0 - b_1 \\bar{x} \\big).\n\\end{equation}\\]\nDerivando SQR em relação a \\(b_1\\) temos\n\\[\\begin{equation}\n\\dfrac{\\partial SQR}{\\partial b_1}= -2 \\displaystyle \\sum_{i=1}^n x_i (y_i - b_0 - b_1 x_i) = -2  \\Big( \\displaystyle \\sum_{i=1}^n x_i y_i - b_0 \\displaystyle \\sum_{i=1}^n x_i - b_1 \\displaystyle \\sum_{i=1}^n x_i^2 \\Big).\n\\end{equation}\\]\nIgualando a zero\nFazendo \\(\\frac{\\partial SQR}{\\partial b_0} = 0\\) e \\(\\frac{\\partial SQR}{\\partial b_1} = 0\\) temos:\n\\[\\underbrace{\\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}}_{A} \\quad e \\quad \\underbrace{\\displaystyle \\sum_{i=1}^n x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0}_{B}\\]\n\nQuando igualamos a zero, substituimos \\(b_0\\) por \\(\\hat{\\beta}_0\\) e \\(b_1\\) por \\(\\hat{\\beta}_1\\) pois \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) são a solução do sistema.\nVamos então resolver o sistema de equações para obter \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\).\nDe \\(A\\) temos que \\(\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\). Agora vamos substituir \\(\\hat{\\beta}_0\\) em \\(B\\),\n\\[\\displaystyle \\sum_{i=1}^n x_i (y_i - \\underbrace{(\\bar{y} - \\hat{\\beta}_1 \\bar{x})}_{\\hat{\\beta}_0} - \\hat{\\beta}_1 x_i) = 0\\] \\[\\displaystyle \\sum_{i=1}^n x_i(y_i-\\bar{y}) - \\hat{\\beta}_1 \\sum_{i=1}^n x_i (x_i - \\bar{x}) = 0\\]\n\\[\\displaystyle \\sum_{i=1}^n x_i(y_i-\\bar{y}) = \\hat{\\beta}_1 \\sum_{i=1}^n x_i (x_i - \\bar{x}),\\] Então, \\[\\hat{\\beta}_1 = \\dfrac{\\displaystyle \\sum_{i=1}^n x_i(y_i-\\bar{y})}{\\displaystyle \\sum_{i=1}^n x_i (x_i - \\bar{x})}.\\] Provavelmente, a fórmula não se parece muito com as fórmulas que você está acostumado a ver nos livros. Vamos trabalhar um pouco com as expressões no numerador e denominador para chegarmos a uma fórmula um pouco mais conhecida.\nNo denominador:\n\\[\\displaystyle \\sum_{i=1}^n x_i (x_i-\\bar{x}) = \\sum_{i=1}^n (x_i - \\bar{x} + \\bar{x})(x_i-\\bar{x}) = \\underbrace{\\sum_{i=1}^n (x_i - \\bar{x})(x_i-\\bar{x})}_{\\displaystyle \\sum_{i=1}^n (x_i - \\bar{x})^2} + \\underbrace{\\sum_{i=1}^n  \\bar{x}(x_i-\\bar{x})}_{\\bar{x} \\displaystyle \\sum_{i=1}^n  (x_i-\\bar{x})},\\] e como \\(\\displaystyle \\sum_{i=1}^n (x_i - \\bar{x}) = \\underbrace{\\sum_{i=1}^n x_i}_{n\\bar{x}} - n \\bar{x} = 0,\\) temos que \\[\\displaystyle \\sum_{i=1}^n x_i (x_i-\\bar{x}) = \\displaystyle \\sum_{i=1}^n (x_i-\\bar{x})^2.\\] No numerador:\n\\[\\displaystyle \\sum_{i=1}^n x_i(y_i-\\bar{y}) = \\displaystyle \\sum_{i=1}^n (x_i - \\bar{x} + \\bar{x})(y_i-\\bar{y}) = \\displaystyle \\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y}) + \\underbrace{\\displaystyle \\sum_{i=1}^n \\bar{x}(y_i-\\bar{y})}_{\\bar{x} \\underbrace{\\displaystyle \\sum_{i=1}^n (y_i-\\bar{y})}_{0}}\\] Então temos que, \\[\\hat{\\beta}_1 = \\dfrac{\\displaystyle \\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\displaystyle \\sum_{i=1}^n (x_i-\\bar{x})^2} \\quad e \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\\]\nVerificando que é ponto de mínimo\nPara verificar que a solução obtida é ponto de mínimo, precisamos que a matriz Hessiana \\[\\begin{bmatrix} \n\\dfrac{\\partial^2 SQR}{\\partial b_0^2} & \\dfrac{\\partial^2 SQR}{\\partial b_0 \\partial b_1}  \\\\\n\\dfrac{\\partial^2 SQR}{\\partial b_1 \\partial b_0} & \\dfrac{\\partial^2 SQR}{\\partial b_1^2}  \\\\\n\\end{bmatrix}\\] seja definida positiva.\n\\(\\dfrac{\\partial^2 SQR}{\\partial b_0^2} = \\dfrac{\\partial}{\\partial b_0}\\dfrac{\\partial SQR}{\\partial b_0} = 2n\\)\n\\(\\dfrac{\\partial^2 SQR}{\\partial b_0 \\partial b_1} = \\dfrac{\\partial}{\\partial b_0}\\dfrac{\\partial SQR}{\\partial b_1} = 2n \\bar{x}\\)\n\\(\\dfrac{\\partial^2 SQR}{\\partial b_1 \\partial b_0} = \\dfrac{\\partial}{\\partial b_1}\\dfrac{\\partial SQR}{\\partial b_0} = 2n \\bar{x}\\)\n\\(\\dfrac{\\partial^2 SQR}{\\partial b_1^2} = \\dfrac{\\partial}{\\partial b_1}\\dfrac{\\partial SQR}{\\partial b_1} = 2 \\displaystyle \\sum_{i=1}^n x_i^2\\)\nLogo, a matriz Hessiana \\[\\begin{bmatrix} \n2n & 2n \\bar{x} \\\\\n2n \\bar{x} & 2 \\displaystyle \\sum_{i=1}^n x_i^2  \\\\\n\\end{bmatrix},\\] é definida positiva pois os elementos da diagonal são positivos e o determinante \\(4n\\displaystyle \\sum_{i=1}^n x_i^2 - 4n^2\\bar{x}^2 = 4n \\Big( \\sum_{i=1}^n x_i^2 - n\\bar{x}^2\\Big) = 4n \\displaystyle \\sum_{i=1}^n(x_i - \\bar{x})^2\\) é também positivo.\nEstimador MQO\nAssim, temos mostrado que os estimadores MQO para \\(\\beta_0\\) e \\(\\beta_1\\) no modelo de regressão linear simples são \\[\\hat{\\beta}_1 = \\dfrac{\\displaystyle \\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\displaystyle \\sum_{i=1}^n (x_i-\\bar{x})^2} \\quad e \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\\]\nConclusões\nO estimador MQO consiste em encontrar \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) que minimizm a soma de quadrados dos resíduos.\nUtilizando os critérios da primeira e segunda derivada temos obtido os estimadore MQO.\nSob algumas hipóteses, os estimadores MQO são o melhor estimador linear não viesado ver Teorem de Gauss-Markov\nSe quiser saber como implementar este modelo em R e Python pode ver este post.\n\nAssumindo que \\(\\mathbb{E}(u|X) = 0\\), \\(\\mathbb{E}(Y|x) = \\beta_0 + \\beta_1 x\\)↩︎\n",
    "preview": "posts/2021-04-01-minimos-quadrados-ordinarios/derivando-o-estimador-de-mqo_files/figure-html5/MQO1-1.png",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-14-book-review-r-for-data-science/",
    "title": "Book Review: R for Data Science",
    "description": "Minhas impressões do livro R for data science: import, tidy, transform, visualize, and model data do Hadley Wickham e Garrett Grolemund.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-03-14",
    "categories": [
      "Book Review",
      "R",
      "rstats"
    ],
    "contents": "\n\nContents\nIntrodução\nOverview do livro\nO que menos gostei\nComentários Finais\n\nIntrodução\nQuem me conhece um pouco, sabe que eu não sou um grande fã de livros do tipo Hands-On blah blah, mas recentemente, buscando ajudar aos meus alunos na sua caminhada acadêmica, resolvi incluir alguns deles na minha lista de leitura. Espero que estes comentários sejam de ajuda, principalmente, para meus (ex-/atuais/futuros) alunos.\nOverview do livro\n\n\n\n\n\nR tem evoluído bastante desde que foi oficialmente lançado em 2001 e o livro R for data science: import, tidy, transform, visualize, and model data (Wickham and Grolemund 2016) faz um bom papel apresentando uma introdução ao R e à filosofia tidyverse1 de forma clara e direta. O pacote (ou, na verdade o conjunto de pacotes) tidyverse é sem dúvida a tendência hoje em dia, e qualquer pessoa que trabalha com dados o utilizará com frequência.\nSugiro que à medida que você for lendo o livro implemente os códigos que forem aparecendo, dessa forma você poderá ir mexendo gradualmente no código para ver o que acontece se… o que lhe ajudara no processo de aprendizagem.\nUm dos capítulos que mais gostei foi o capítulo 3, que apresenta de forma bem leve uma introdução ao pacote ggplot2 para visualização de dados. Um bom complemento para esse capítulo aparece quase no final do livro, no capítulo 28, onde se apresentam alguns detalhes sobre títulos, captions e nomes nos eixos. Se você tiver interesse em se aprofundar no ggplot2, a melhor fonte é o livro ggplot2: Elegant Graphics for Data Analysis (Hadley 2016) que está disponível online e de graça aqui.\nOs capítulos 9 – 16 apresentam bastante material sobre manipulação de dados, super útil para construir nossa ABT2. Contudo, creio que quem não está muito acostumado com o R ou com manipulação de dados pode ter uma overdose de informação. Não se preocupe tanto por entender tudo que está no livro, mas por entender o que pode ser feito com o R e com os pacotes dplyr,readr, lubridate, etc, incluidos no tidyverse. Existem diversos Cheatsheets que ajudam a lembrar como cada umas das funções discutidos nos capítos 9–16 funcionam, salve eles no computador e tenha-os sempre por perto.\nOutro capítulo que achei muito interessante é o capítulo 25 (mas para quem está iniciando eu recomendaria pular esse capitulo e voltar nele quando for um usuário de R mais frequente), ele apresenta informação valiosa para quem tem interesse em comparar vários modelos e colocar modelos em produção.\nO que menos gostei\nEmbora eu tenha desfrutado bastante meu tempo lendo o livro, achei os capítulos 22–24 meio confussos, principalmente para quem está iniciando. Quando se trata de modelagem , eu prefiro uma abordagem mais clássica onde se explica como o modelo é construido e quais são os princípios por tras dele, mas entendo que isso está completamente fora do escopo do livro.\nPara quem está começando, eu leria o capítulo 21 apenas até a seção 21.3. As seções 21.4 – 21.9 são importantes, mas eu deixaria elas para uma segunda leitura ou para quando estiver mais familiarizado com o R e com programação.\nComentários Finais\nResumindo, R for data science: import, tidy, transform, visualize, and model data (Wickham and Grolemund 2016) é um bom livro, completo e didático. Eu gostei da maioria de capítulos, com algumas poucas exceções.\nO livro tem uma versão em português, mas eu li a versão em inglês (que é disponibilizada gratuitamente pelos autores).\nSe tiver com dificuldade em resolver os exercícios do livro, Jeffrey B. Arnold providenciou um solucionário (eu não o li).\n\n\n\nHadley, Wickham. 2016. ggplot2: Elegrant Graphics for Data Analysis. Springer.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\".\n\n\nConjunto de pacotes que seguem a mesma filosofia tidy, para mais detalhes veja tidyverse.org↩︎\nABT: Analytical Base Table↩︎\n",
    "preview": "posts/2021-03-14-book-review-r-for-data-science/cover.png",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {},
    "preview_width": 500,
    "preview_height": 750
  },
  {
    "path": "posts/2021-02-28-teorema-de-gauss-markov/",
    "title": "Teorema de Gauss-Markov",
    "description": "Uma das propriedades mais interessentes dos estimadores MQO é fornecida pelo Teorema de Gauss-Markov. Neste post discutimos a importância, significado e fornecemos uma demostração passo a passo do Teorema.",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-02-28",
    "categories": [
      "Linear Regression",
      "Proof"
    ],
    "contents": "\n\nContents\nIntrodução\nTeorema\nDemostração\nConclusão\n\nIntrodução\n\n\n\nO estimador de mínimos quadrados ordinários (MQO) é um dos métodos de estimação mais utilizados quanto à analise de regressão se refere. Ele é atrativo pela sua simplicidade e boas propriedades.\nSejam \\(\\{(y_i, x_{i,1}, \\ldots, x_{i,k}) \\}_{i=1, \\ldots, n}\\) tais que:\n\\[\\begin{align}\n\\begin{split}\\label{eq:1}\n    y_1 &= \\beta_0 + \\beta_1 x_{1,1}  + \\cdots + \\beta_k x_{1,k} + u_1 \\\\\n    \\vdots \\\\\n    y_n &= \\beta_0 + \\beta_1 x_{n,1}  + \\cdots + \\beta_k x_{n,k} + u_n \\\\\n\\end{split}\n\\end{align}\\]\nou equivalentemente\n\\[ \\underbrace{\\left[ \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right]}_{Y} = \\underbrace{\\begin{bmatrix} \n1 & x_{1,1} & \\cdots & x_{1,k} \\\\ \n\\vdots & \\vdots & \\cdots & \\vdots \\\\ \n1 & x_{n,1} & \\cdots & x_{n,k} \\end{bmatrix}}_{X} \\times \\underbrace{\\left[ \\begin{array}{c} \\beta_0 \\\\ \\vdots \\\\ \\beta_k \\end{array} \\right]}_{\\beta} + \\underbrace{\\left[ \\begin{array}{c} u_1 \\\\ \\vdots \\\\ u_n \\end{array} \\right]}_{u}\\]\nentão, o estimador MQO de \\(\\beta\\) é dado por \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\).\nSob certas condições, o Teorema de Gauss-Markov nos diz que \\(\\hat{\\beta}\\) é o melhor estimador linear não viesado (BLUE em Inglês):\nEle é linear1 pois \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\) (basta fazer \\(A' = (X'X)^{-1}X'\\)).\nEle é não viesado pois \\(\\mathbb{E}(\\hat{\\beta}) = \\beta\\) e\nEle é o melhor, pois possui a menor variância entre todos os outros estimadores lineares não viesados.\nTeorema\nSeja \\(Y = X \\beta + u\\) com \\(X\\) de posto completo, \\(\\mathbb{E}(u|X) = 0\\) e \\(\\mathbb{V}(u|X) = \\sigma^2 I\\). Então \\(\\hat{\\beta}\\), o estimador MQO de \\(\\beta\\), é o melhor estimador linear não viesado (BLUE) de \\(\\beta\\).\nNote que o Teorema requer que:\nO modelo populacional seja da forma \\(Y = X \\beta + u\\)\n\\(X\\) seja de posto completo (ou seja não existe colineariedade perfeita)\n\\(\\mathbb{E}(u|X) = 0\\)\n\\(\\mathbb{V}(u|X) = \\sigma^2 I\\)\nEssas condições são às vezes conhecidas como as hipóteses de Gauss–Markov. Se alguma das hipóteses de Gauss–Markov não for valida, então \\(\\hat{\\beta}\\) não será mais BLUE.\n\nOu seja Teorema de Gauss-Markov nos diz que se as condições do Teorema forem satisfeitas, não adianta buscar por algum outro estimador linear não viesado, pois \\(\\hat{\\beta}\\) será o melhor (de menor variância).\n\nDemostração\nSeja \\(\\tilde{\\beta}\\) qualquer outro estimador linear não viesado de \\(\\beta\\).\nComo \\(\\tilde{\\beta}\\) é um estimador linear, ele é da forma \\(\\tilde{\\beta} = A'Y\\) (para qualquer matriz \\(A\\) de dimensão \\(n \\times k+1\\) função de \\(X\\).).\nComo \\(\\tilde{\\beta}\\) é não viesado, temos que \\(A'X = I\\) pois \\[\\begin{equation}\n\\begin{aligned}\n\\mathbb{E}(\\tilde{\\beta} | X)  & =  \\mathbb{E}(A'Y | X)\\\\\n                   & =  \\mathbb{E}(A' (X\\beta + u) | X) \\\\\n                   & =  \\mathbb{E}(A'X\\beta|X) + \\mathbb{E}(u|X) \\\\\n                   & =  \\mathbb{E}(A'X\\beta|X) \\quad \\text{pois } \\mathbb{E}(u|X) = 0 \\\\\n                   & =  A'X \\beta, \n\\end{aligned}\n\\end{equation}\\] que é não viesado se e somente se \\(A'X = I\\)\nA variância de \\(\\tilde{\\beta}\\) (condicional em \\(X\\)) é \\[\\begin{equation}\n\\begin{aligned}\n\\mathbb{V}(\\tilde{\\beta}|X) & =  \\mathbb{V}(A'Y|X) \\\\\n                & =  A'\\mathbb{V}(Y|X)A \\\\\n                & =  A'\\mathbb{V}(X \\beta + u|X)A \\\\\n                & =  A'\\mathbb{V}(u|X)A \\\\\n                & =  A'\\sigma^2 I A \\\\\n                & =  \\sigma^2 A'A \\\\\n\\end{aligned}\n\\end{equation}\\]\nDefinamos \\(C = A - X(X'X)^{-1}\\), então \\(X'C = \\underbrace{X'A}_{I}-\\underbrace{X'X(X'X)^{-1}}_{I} = 0\\)\nSabemos que \\(\\mathbb{V}(\\hat{\\beta}|X) = \\sigma^2 (X'X)^{-1}\\). Então basta provar que \\(\\mathbb{V}(\\tilde{\\beta}|X) - \\mathbb{V}(\\hat{\\beta}|X)\\) é semi-definida positiva, ou seja \\[A'A-(X'X)^{-1} \\geq 0.\\] Para provar isto vejamos que \\[\\begin{equation}\n\\begin{aligned}\nA'A-(X'X)^{-1} & =  [C+X(X'X)^{-1}]'[C+X(X'X)^{-1}] -(X'X)^{-1} \\\\\n            & =  [C'+ (X'X)^{-1} X'] [C+X(X'X)^{-1}] -(X'X)^{-1} \\\\\n            & =  C'C +  \\underbrace{C' X}_{0}(X'X)^{-1} + (X'X)^{-1} \\underbrace{X'C}_{0} \\\\ &  + (X'X)^{-1} \\underbrace{X' X(X'X)^{-1}}_{I} -(X'X)^{-1}\\\\\n            & =  C'C + (X'X)^{-1} -(X'X)^{-1} \\\\\n            & =  C'C \\geq 0.\\\\\n\\end{aligned}\n\\end{equation}\\]\nCom isso temos provado que \\(A'A \\geq (X'X)^{-1}\\) ou equivalentemente, \\[\\underbrace{\\sigma^2 A'A}_{\\mathbb{V}(\\tilde{\\beta}|X)} \\geq \\underbrace{\\sigma^2 (X'X)^{-1}}_{\\mathbb{V}(\\hat{\\beta}|X)},\\] que é o que queremos demostrar.\n\\(C'C\\) é de fato semi-definida positiva, veja Teorema A.4 em (Hansen 2020) ou 10.10 em (Seber 2008).\nConclusão\nSob as hipóteses de Gauss-Markov, temos demostrado que o estimador de MQO, \\(\\hat{\\beta}\\), amplamente utilizado em análise de regressão é o melhor estimador linear não viesdado. Isto significa que se as hipóteses de Gauss-Markov são verificadas, não conseguiremos um estimador linear que seja melhor (menor variância) do que \\(\\hat{\\beta}\\).\n\n\n\nHansen, Bruce E. 2020. Econometrics. Online version. Wisconsin.\n\n\nSeber, George AF. 2008. A Matrix Handbook for Statisticians. Vol. 15. John Wiley & Sons.\n\n\nUm estimador linear é um estimador da forma \\(\\tilde{\\beta} = A'Y\\) para uma matriz \\(A\\) de dimensão \\(n \\times k+1\\) função de \\(X\\).↩︎\n",
    "preview": {},
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-25-intro-regressao-linear/",
    "title": "Intro à Regressão Linear",
    "description": "Uma breve introdução à Análise de Regressão Linear: interpretação e implementação no R (e no Python).",
    "author": [
      {
        "name": "Carlos Trucíos",
        "url": "https://ctruciosm.github.io"
      }
    ],
    "date": "2021-02-25",
    "categories": [
      "Linear Regression",
      "R",
      "Python",
      "rstats"
    ],
    "contents": "\n\nContents\nIntrodução\nEstimação por MQO\nImplementação no R\nInterpretação\nConclusões\nBonus\n\nIntrodução\nUma das técnicas mais conhecidas e difundidas no mundo de statistical/machine learning é a Análise de Regressão Linear (ARL). Ela é útil quando estamos interessados em explicar/predizer a variável dependente \\(y\\) utilizando um conjunto de \\(k\\) variaveis explicativas \\(x_1, \\ldots, x_k\\).\nBasicamente, utilizamos as \\(k\\) variáveis explicativas para entender o comportamento de \\(y\\) e, num contexto de regressão linear, assumimos que a relação entre \\(y\\) e as \\(x\\)’s é dada por uma função linear da forma:\n\\[y = \\underbrace{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k}_{f(x_1, \\ldots, x_k)} + u,\\] em que \\(u\\) é o termo de erro.\nEstimação por MQO\nNa prática, nunca conhecemos \\(\\beta = [\\beta_0, \\beta_1, \\ldots, \\beta_k]'\\) e temos que estima esses valores utilizando os dados. Existem diferentes métodos de estimação, sendo o método de mínimos quadraros ordinários (MQO) um dos mais comumente utilizados1.\nO estimador de MQO é dado por \\[\\hat{\\beta} = (X'X)^{-1}X'Y,\\] e sua respectiva matriz de covariância (condicional em \\(X\\)) é dada por \\[V(\\hat{\\beta}|X) = \\sigma^2(X'X)^{-1}\\] em que \\(Y = [y_1, \\ldots, y_n]'\\) e \\(X = \\begin{bmatrix} 1 & x_{1,1} & \\cdots & x_{1,k} \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_{n,1} & \\cdots & x_{n,k} \\end{bmatrix}\\)\n\\(\\sigma^2\\) nunca é conhecido, então utilizamos \\(\\hat{\\sigma}^2 = \\dfrac{ \\sum_{i=1}^n \\hat{u}_i^2}{n-k-1}\\), que é um estimador não viesado de \\(\\sigma^2\\) (\\(E(\\hat{\\sigma}^2) = \\sigma^2\\)).\nAssim, na prática nós sempre utilizamos \\(\\widehat{V}(\\hat{\\beta}|X) = \\hat{\\sigma}^2(X'X)^{-1}\\) no lugar de \\(V(\\hat{\\beta}|X)\\).\nO desvio padrão, geralmente reportados pelos softwares estatísticos/econométricos, é a raiz quadrada dos elementos na diagonal de \\(\\widehat{V}(\\hat{\\beta}|X)\\).\nO Teorema de Gaus–Markov estabelece que, sob algumas hipóteses (conhecidas como as hipóteses de Gauss-Markov), \\(\\hat{\\beta}\\) é o melhor estimador linear não viesado (BLUE em inglês: Best Linear Unbiased Estimator), ou seja, para qualquer outro estimador linear2 \\(\\tilde{\\beta}\\), \\[V(\\tilde{\\beta}|X) \\geq V(\\hat{\\beta}|X).\\]\nA Figura 1 mostra um exemplo de uma reta de regressão \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) obtida pelo método de MQO.\n\n\n\nFigure 1: OLS regression line example\n\n\n\nImplementação no R\nRealizar uma regressão linear no R não é difícil, para ver como faze-lo utilizaremos o conjunto de dados hprice1 disponível no pacote wooldridge do R.\nSe assumirmos que o modelo populacional é da forma \\[price = \\beta_0 + \\beta_1 bdrms + \\beta_2 lotsize +  \\beta_3 sqrft + \\beta_4 colonial + u,\\] utilizamos o seguintes comandos\n\n\nlibrary(wooldridge)\nmodelo = lm(price~bdrms+lotsize+sqrft+colonial, data = hprice1)\nmodelo\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nCoefficients:\n(Intercept)        bdrms      lotsize        sqrft     colonial  \n -24.126528    11.004292     0.002076     0.124237    13.715542  \n\nO output anterior apenas mostra os \\(\\hat{\\beta}\\)’s, um output mais completo, que inclui o desvio padrão dos \\(\\hat{\\beta}\\)’s, o teste T, teste F, \\(R^2\\) e p-valores pode ser facilmente obtido utilizando a função summary( ).\n\n\nsummary(modelo)\n\n\n\nCall:\nlm(formula = price ~ bdrms + lotsize + sqrft + colonial, data = hprice1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.268  -38.271   -6.545   28.210  218.040 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.413e+01  2.960e+01  -0.815  0.41741    \nbdrms        1.100e+01  9.515e+00   1.156  0.25080    \nlotsize      2.076e-03  6.427e-04   3.230  0.00177 ** \nsqrft        1.242e-01  1.334e-02   9.314 1.53e-14 ***\ncolonial     1.372e+01  1.464e+01   0.937  0.35146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 59.88 on 83 degrees of freedom\nMultiple R-squared:  0.6758,    Adjusted R-squared:  0.6602 \nF-statistic: 43.25 on 4 and 83 DF,  p-value: < 2.2e-16\n\nInterpretação\nAntes de interpretar os resultados é importante e necessário conhecer os dados e saber quais as unidades de medida das nossas variáveis3.\nPara darmos uma olhada nos dados utilizaremos as funções select( ) e glimpse( ) do pacote dplyr.\n\n\nlibrary(dplyr)\nhprice1 %>% select(price, bdrms, lotsize, sqrft, colonial) %>% glimpse()\n\n\nRows: 88\nColumns: 5\n$ price    <dbl> 300.000, 370.000, 191.000, 195.000, 373.000, 466.27…\n$ bdrms    <int> 4, 3, 3, 3, 4, 5, 3, 3, 3, 3, 4, 5, 3, 3, 3, 4, 4, …\n$ lotsize  <dbl> 6126, 9903, 5200, 4600, 6095, 8566, 9000, 6210, 600…\n$ sqrft    <int> 2438, 2076, 1374, 1448, 2514, 2754, 2067, 1731, 176…\n$ colonial <int> 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, …\n\n\nO pacote dplyr é utilizado para manipulação de dados. A função select( ) seleciona algumas das variaveis contidas em hprice1 e a função glimpse ( ) nos permite ver rapidamente a estrutura dos dados.\nA descrição das variáveis é apresentada a seguir e ela pode ser obtida utilizando diretamente os comandos help(hprice1) ou ?hprice1.\nVariável\nDescrição\nprice\npreço da casa (em milhares de dólares)\nbdrms\nnúmero de quartos\nlotsize\ntamanho do lote da casa (em pés\\(^2\\))\nsqrft\ntamanho da casa (em pés\\(^2\\))\ncolonial\nDummy (=1 se a casa for de estilo colonial)\nConhecendo melhor os dados, vamos então interpretar os resultados:\n\\(\\approx 66\\%\\) da variabilidade do preço (price) é explicada pelo nosso modelo4.\nUtilizando as estatísticas T (\\(H_0: \\beta_i = 0 \\quad \\text{vs.} \\quad H_1: \\beta_i \\neq 0\\)) apenas lotsize e sqrft são estatísticamente significativas (i.e. rejeitamos \\(H_0\\)) ao nível de significância de 5%.\nO incremento em 481 pés\\(^2\\) (pés quadrados) no lote da casa, implica, em média, um incremento de mil dólares5 no preço da casa (permanecendo fixos os outros fatores).\nO incremente em 8 pés\\(^2\\) no tamanho da casa, implica, em édia, um incremento de mil dólares6 no preço da casa (permanecendo fixos os outros fatores).\nFinalmente, summary( ) também fornece informação para testar conjuntamente \\[H_0: \\beta_{bdrms}=0,\\beta_{lotsize}=0,\\beta_{sqrft}=0,\\beta_{colonial}=0\\] vs. \\[H_1: H_0 \\text{ is not true. }\\] Utilizando o teste F, rejeitamos \\(H_0\\) (p-valor \\(\\approx\\) 0, F-statistics = 43.25)\n\nObviamente, nossa interpretação foi realizada assumindo que as hipóteses do modelo linear clássico são satisfeitas. Se as hipóteses não são satisfeitas, precisamos melhoras/corrigir nosso modelo e apenas interpretar os resultados quando as hipóteses do modelo linear clássico forem verificadas.\n\nNo livro do Wooldridge7 encontramos uma interessante discussão sobre como interpretar os \\(\\beta\\)’s quando utilizamos ou não transformações logaritmicas. A seguinte Tabela apresenta um resumo dessa discussão e fornece uma guia para melhor interpretarmos os resultados\nVariável dependente\nVariável independente\nInterpretação do \\(\\beta\\)\n\\(y\\)\n\\(x\\)\n\\(\\Delta y = \\beta \\Delta x\\)\n\\(y\\)\n\\(\\log(x)\\)\n\\(\\Delta y = \\big(\\beta/100 \\big) \\% \\Delta x\\)\n\\(\\log(y)\\)\n\\(x\\)\n\\(\\% \\Delta y = 100\\beta \\Delta x\\)\n\\(\\log(y)\\)\n\\(\\log(x)\\)\n\\(\\% \\Delta y = \\beta \\% \\Delta x\\)\nConclusões\nARL é um métodos estatístico (econométrico, de machine/statiscal learning) poderoso e facil de implementar, ele pode fornecer insights importantes sobre nossos dados (e consequentemente sobre nosso negócio).\nR fornece uma forma facil de utilizar regressão linear e fornece também informação útil para sua interpretação. Contudo, é importante tomar cuidado sobre as hipóteses assumidas no modelo (que é o tópico do nosso próximo post), a não verificação das hipóteses pode ter um forte impacto nos resultados obtidos.\nBonus\nImplementação em Python\n\nimport statsmodels.api as sm\nimport pandas as pd\nfrom patsy import dmatrices\n\nurl = \"https://raw.githubusercontent.com/ctruciosm/statblog/master/datasets/hprice1.csv\"\nhprice1 = pd.read_csv(url)\n\ny, X = dmatrices('price ~ bdrms + lotsize + sqrft + colonial', \n                  data = hprice1, return_type = 'dataframe')\n# Definir o modelo\nmodelo = sm.OLS(y, X)\n# Ajustar (fit) o modelo\nmodelo_fit = modelo.fit()\n# Resultados completos do modelo\nprint(modelo_fit.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.676\nModel:                            OLS   Adj. R-squared:                  0.660\nMethod:                 Least Squares   F-statistic:                     43.25\nDate:                Sáb, 24 Jul 2021   Prob (F-statistic):           1.45e-19\nTime:                        10:24:23   Log-Likelihood:                -482.41\nNo. Observations:                  88   AIC:                             974.8\nDf Residuals:                      83   BIC:                             987.2\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -24.1265     29.603     -0.815      0.417     -83.007      34.754\nbdrms         11.0043      9.515      1.156      0.251      -7.921      29.930\nlotsize        0.0021      0.001      3.230      0.002       0.001       0.003\nsqrft          0.1242      0.013      9.314      0.000       0.098       0.151\ncolonial      13.7155     14.637      0.937      0.351     -15.397      42.828\n==============================================================================\nOmnibus:                       24.904   Durbin-Watson:                   2.117\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               45.677\nSkew:                           1.091   Prob(JB):                     1.21e-10\nKurtosis:                       5.774   Cond. No.                     6.43e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.43e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nA ideia básica deste método é encontrar os valores \\(\\hat{\\beta}\\)’s que minimizam a soma de quadrados dos residuos.↩︎\nUm estimador linear é um estimador da forma \\(\\tilde{\\beta} = A'Y\\) em que a matrix \\(A\\) é uma matriz de dimensão \\(n \\times k+1\\) função de \\(X\\)↩︎\nNa prática, antes de ajustar a reta de regressão é feita uma análise exploratória de dados (EDA em inglês). Nessa EDA já conheceremos melhor as variáveis com as que estamos trabalhando, bem como as unidades de medida.↩︎\nGeralmente, preferimos o \\(R^2_{Adjusted}\\) ao \\(R^2\\)↩︎\n\\(2.076e-03*481 = 0.998556 \\approx 1\\)↩︎\n\\(0.1242*8 = 0.9936 \\approx 1\\)↩︎\nWooldridge, J. M. (2016). Introdução à Econometria: Uma abordagem moderna. Cengage.↩︎\n",
    "preview": "posts/2021-02-25-intro-regressao-linear/uma-introduo-regresso-linear_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2024-09-28T01:04:54-03:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  }
]
